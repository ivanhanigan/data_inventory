#+TITLE:Data Inventory 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----


* COMMENT todo list
** TODO sqlite-code
#+name:sqlite
#+begin_src R :session *shell* :tangle no :exports none :eval no
  #### name:sqlite ####
  library(RSQLite)
  drv <- dbDriver("SQLite")
  con <- dbConnect(drv, dbname = "~/tools/web2py/applications/data_inventory/databases/storage.sqlite")
  dbListTables(con)
  dbGetQuery(con , "select * from dataset")[,1:5]
  dbGetQuery(con , "select * from entity")[,1:5]
  
  
#+end_src

    


** TODO add for codes
- 040107 - Meteorology
- 111706 Epidemiology

** TODO add to methods protocol
current - Study type: select from a list like time-series, cross-sectional, spatial, exploratory, confirmatory

A Sleigh
Type of study
Experimental vs natural, descriptive vs analytical (trial, cohort, case-control, prevalence, ecological, case-report, etc).  If case-control or cohort, was the timing of data collection retrospective or prospective?



* COMMENT Init
** COMMENT torun-code
#+name:torun
#+begin_src sh :session *shell* :tangle no :exports none :eval no
#### name:torun####
# - include the server config: 
python ~/tools/web2py/web2py.py -a xpassword -i 0.0.0.0 -p 8181
#+end_src

** COMMENT README.md-code
#+name:README.md
#+begin_src R :session *R* :tangle ../README.md :exports none :eval no
  #### Data Inventory
  
  Licence: CC-BY
  
  ## About
  
  1. A web2py app to help manage research data
  1. Designed for the Ecology discipline, using Ecological Metadata Language standard concepts
  1. Can run as simple standalone desktop app using sqlite or shared on a postgres server for concurrent access by a team
  1. Highly customised for the Oz Long Term Ecological Research Network Data Portal's needs
  
  ## To install using R:
  
  ```{r}
  setwd("~")
  if(!require(downloader)) install.packages("downloader"); require(downloader)
  download("https://raw.githubusercontent.com/ivanhanigan/data_inventory/master/static/install.R",
           "install.R", mode = "wb")
  source("install.R")
  ```
  
  ### or alternatively do a manual install
  
  1. Download web2py http://www.web2py.com/init/default/download 
  1. Put all the files into your web2py/applications as 'data_inventory'
  1. Dbl-Click to Run the wep2py.py file and go to 127.0.0.1:8000/data_inventory
  1. README is at http://127.0.0.1:8181/data_inventory/static/index.html
  
#+end_src
* Introduction
- There is a need for a data inventory tool to enable ecological data collections to be managed more efficiently
- The Ecological Metadata Language (EML) provides an excellent standardised schema for metadata, but the associated software Morpho does not have the flexibility or extensibility that would enable a data manager to use it to manage large and heterogeneous data collections
- This article describes an approach to use a relational database with web-based forms (using the web2py framework) and helper functions (written in the R and SQL languages) 
** The Ecological Metadata Language (EML)
The  Ecological Metadata Language (EML) schema is defined by the 'Knowledge Network for Biocomplexity' - KNB (https://knb.ecoinformatics.org/#tools/eml).  KNB is a subproject of  Ecoinformatics.org (http://ecoinformatics.org/).  Ecoinformatics.org is an open, voluntary collaboration of developers and researchers that aims to produce software, systems, publications, and services that are beneficial to the ecological and environmental sciences. 
*** COMMENT snip
- We follow the LTER descriptions of best practice "~/references/LTER - Unknown - Introduction What is EML.pdf"
- We added some things like metadataProvider, but this is optional

*** XML 
A minimal EML document is summarised below.  This is a schematic view that shows the structure of an EML XML document.  Only a few of the possible components are included.  


#+begin_src R :session *R* :tangle no :exports reports :eval no
    <eml>
        <dataset>
            <title>
            <creator> 
            <contact> 
            <project> 
                <title> 
                <personnel>
                    <role>
                <abstract>
                <funding>
                <studyAreaDescription>
                <designDescription>
                <relatedProject>
            <associatedParty>
            <publisher>
            <pubDate>
            <keywords>
            <abstract> 
            <intellectualright>
            <methods> 
            <coverage>
            <dataTable>
                <entityName>
                <entityDescription>                
                <physical>
                <attribute>
        <additionalMetadata>
            <additionalLinks>
  
#+end_src
*** A schematic diagram of the EML modules
In the figure below the EML hierarchy of modules is shown graphically. An EML metadata document contains a dataset module that allows the grouping of one or more data entities linked together by particular integrity constraints to form a Data Package.  The project module places the dataset that is being documented into its larger research context, and optionally allows multiple related Data Packages to be linked. The mandatory component of the EML is shown as a firm line, in contrast with those aspects of the EML which are not mandatory (dotted lines).

[[file:./images/EML_project.png]]

*** The parts of an EML document 
In EML, the term dataset is applied to a document which may contain
  general information such as the title, creator, and contacts,
  abstract and keywords as well as one or more data entities, such as
  datatables, that provide more specific research details.

The information is at the level of a particular data collection or
  study, however there is also a level above this at the "project"
  level where research studies can be grouped together.

In EML, the term dataset refers to one or more data
  entities.  However, there is no generally accepted practice on what to
  include in an EML-dataset document. Some people will choose to have
  a one-to-one correspondence between an EML document and a data
  entity and a single physical file. Others will document several data
  entities in one dataset document. 

 Below is a brief description of the way we interpret the components of an EML dataset

**** Datasets
Dataset is the top level of the EML.  The miminum necessary to
produce a valid EML document is <title>, <creator>, <contract>. But
one can't tell very much with just a title and a name.  Our aim is to
create an EML document that contains enough information about the data
and research to make funding agencies happy and give other ecologists
something to work with.

Datasets may include one or more data entities such as data tables and spatial images along with associated metadata, but inclusion of data entities are not mandatory. The dataset module enables entities which are linked together to be grouped logically.  LTERN groups entities into a single dataset using a range of integrity constraints such as the use of a specific sampling structure or monitoring theme employed over a discrete temporal and geographic range.
In the context of the work undertaken by the LTERN Data Portal Team, the term dataset is sometimes used interchangeably with the term “data package”, however it should be recognised that these are not exactly the same.  The dataset module is a part of an EML document which forms the basis for a data package.  

In EML, the dataset tag contains general information about a dataset; at minimum this must comprise a title, one Data Creator and a contact (often the Data Creator and contact will be the same).  Additional information such as abstract, temporal coverage, keywords, geographic coverage and methods are used to provide context for the published or archived data packages. 

For more information about the general EML descriptions see:
https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-resource.html

For more detailed information about the EML-dataset module see: https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-dataset.html
**** Projects
The project module is optional, but in many cases highly desirable as
this provides an overall description of the larger-scale project or
research context with which that dataset is associated.  For example
it might be an entire Longterm Ecological Research Network (LTER)
site, or an individual project at an LTER site.

The EML “project” module of the EML enables data packages to be grouped logically together. The project module is primarily used by LTERN to identify the Data Owner and the source of funding.
The project module is recursive in that it provides an overall description of the larger-scale project or research context for the specific project with which that data package is associated.  Information in the project tag of the EML include detailed descriptions of the aims and objectives of the research, funding sources, personnel and a description of the study area.  This module also includes content on the study and sampling design and the methods employed during the research.  As an example, a project might comprise an entire Long Term Ecological Research Network (LTERN) plot network or an individual project at an LTERN site.

For more information about the EML-dataset module see:
https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-project.html

**** Data Entity
As stated earlier, a dataset consists of one or more data entities,
and the most common data entity is a <dataTable>. A data table is
something that looks like a matrix with rows for observations and
columns for variables

In addition to datatables, people using database applications may also
  produce a <view> from a database management system or a
  <storedProcedure> that results in data output. People using GIS
  (geographical information system) applications generate both
  <spatialVector>, also referred to as boundary or shape files, and
  <spatialRaster>. A <spatialRaster> is a geo-referenced image usually
  produced by a camera on a satellite or other remote sensing
  device. The final kind of data entity is <otherEntity>. An
  <otherEntity> is a data entity that cannot be represented by any of
  the previously defined data entity structures. A non-geo-referenced
  photograph is an <otherEntity>, e.g., a photograph of two different
  types of butterflies.

For more information about types of EML-entities see: https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/#N10115

**** Attributes
An attribute or as it is sometime referred to, variable, in this
  context refers to the name of the column of a datatable

For more information about the EML dataTable type (especially its attributes) see:
https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-dataTable.html#attribute

*** COMMENT snip 
This is where Morpho and other EML tools come into their own.  Our
  database solution might be useful as an initial stage of data
  documentation, to be further refined using specialised EML tools

* COMMENT web2py 
** set up web2py 
- Web2py comes packaged up with everything it needs to run on a system (just needs python)
- run this using:
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
#### Code:
    python ~/tools/web2py/web2py.py -a xpassword -i 0.0.0.0 -p 8181
#+end_src

** the web2py database example
- once the web server is running the example can be visited at this link:
- [[http://127.0.0.1:8181/examples/default/examples#database\_examples]]
- The default configuration of web2py uses the SQLite database engine
- Our implementation also utilises the PostgreSQL database which should be installed separately
** installer
*** COMMENT install
#+name:install
#+begin_src R :session *R* :tangle install.R :exports none :eval no
  #### name:install ####
  # download web2py
  LinuxOperatingSystem <- function(){
      if(length(grep('linux',sessionInfo()[[1]]$os)) == 1)
      {
        #print('Linux')
        os <- 'linux' 
        OsLinux <- TRUE
      }else if (length(grep('ming',sessionInfo()[[1]]$os)) == 1)
      {
        #print('Windows')
        os <- 'windows'
        OsLinux <- FALSE
      }else
      {
        # don't know, do more tests
        print('Non linux or windows os detected. Assume linux-alike.')
        os <- 'linux?'
        OsLinux <- TRUE
      }
     
      return (OsLinux)
    }
  if(LinuxOperatingSystem()){
  download.file("http://web2py.com/examples/static/web2py_src.zip", 
                destfile = "~/web2py_src.zip", mode = "wb")
  unzip("~/web2py_src.zip")
  } else {
  download.file("http://web2py.com/examples/static/web2py_win.zip", 
                destfile = "~/web2py_win.zip", mode = "wb")
  unzip("~/web2py_win.zip")
  }
  
  setwd("~/web2py/applications/")
  downloader::download("https://github.com/ivanhanigan/data_inventory/archive/master.zip", 
           "temp.zip", mode = "wb")
  unzip("temp.zip")
  file.rename("data_inventory-master", "data_inventory")
  setwd("~/web2py/")
  #dir()
  
  if(LinuxOperatingSystem()){
    system("python web2py.py -a xpassword -i 0.0.0.0 -p 8181", wait = F)
  } else {
    system("web2py.exe -a xpassword -i 0.0.0.0 -p 8181", wait = F)
  }
  browseURL("http://127.0.0.1:8181/data_inventory")    
  
#+end_src

* COMMENT Design the database 
** Create a new database
- create a new postgres database using the following  code
#+name:create.db
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
  sudo su
  su - postgres 
  createdb data_inventory
  psql -d data_inventory
  CREATE ROLE w2p_user LOGIN PASSWORD 'xpassword';
  grant all on schema public to w2p_user;
  \q
#+end_src
** Create a new web2py application
- this will create a directory in the applications folder of the web2py home directory
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
  cd ~/tools/web2py 
  python ./web2py.py -S data_inventory
#+end_src

** relational model
- The structure we decided on was that there would be ONE project TO MANY datasets, ONE dataset to MANY datatables, and ONE datatable TO MANY attributes/variables
** defaults for models/db.py
- the database tables are set up in the models/db.py file that comes with default settings
- The first bit to change is the db reference from SQLite to postgres
- also note that "#if request.is\_local else []" will allow using as a server
- Then add new table definitions down the bottom.  
- Here we added projects, datasets, datatables and attributes.

# DISABLED DURING POSTGRES DEV 
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports none :eval no :padline no
  # -*- coding: utf-8 -*-
  
  #########################################################################
  ## This scaffolding model makes your app work on Google App Engine too
  ## File is released under public domain and you can use without limitations
  #########################################################################
  
  ## if SSL/HTTPS is properly configured and you want all HTTP requests to
  ## be redirected to HTTPS, uncomment the line below:
  # request.requires_https()
  
  if not request.env.web2py_runtime_gae:
      ## if NOT running on Google App Engine use SQLite or other DB
      ##db = DAL('sqlite://storage.sqlite',pool_size=1,check_reserved=['all'], fake_migrate_all = False)
      db = DAL("postgres://w2p_user:xpassword@localhost:5432/data_inventory_hanigan_dev4", fake_migrate_all = False)
  else:
      ## connect to Google BigTable (optional 'google:datastore://namespace')
      db = DAL('google:datastore')
      ## store sessions and tickets there
      session.connect(request, response, db=db)
      ## or store session in Memcache, Redis, etc.
      ## from gluon.contrib.memdb import MEMDB
      ## from google.appengine.api.memcache import Client
      ## session.connect(request, response, db = MEMDB(Client()))
  
  ## by default give a view/generic.extension to all actions from localhost
  ## none otherwise. a pattern can be 'controller/function.extension'
  response.generic_patterns = ['*'] # if request.is_local else []
  ## (optional) optimize handling of static files
  # response.optimize_css = 'concat,minify,inline'
  # response.optimize_js = 'concat,minify,inline'
  ## (optional) static assets folder versioning
  # response.static_version = '0.0.0'
  #########################################################################
  ## Here is sample code if you need for
  ## - email capabilities
  ## - authentication (registration, login, logout, ... )
  ## - authorization (role based authorization)
  ## - services (xml, csv, json, xmlrpc, jsonrpc, amf, rss)
  ## - old style crud actions
  ## (more options discussed in gluon/tools.py)
  #########################################################################
  
  from gluon.tools import Auth, Crud, Service, PluginManager, prettydate
  auth = Auth(db)
  crud, service, plugins = Crud(db), Service(), PluginManager()
  
  ## create all tables needed by auth if not custom tables
  auth.define_tables(username=False, signature=False)
  
  ## configure email
  mail = auth.settings.mailer
  mail.settings.server = 'logging' or 'smtp.gmail.com:587'
  mail.settings.sender = 'you@gmail.com'
  mail.settings.login = 'username:password'
  
  ## configure auth policy
  auth.settings.registration_requires_verification = False
  auth.settings.registration_requires_approval = False
  auth.settings.reset_password_requires_verification = True
  
  ## if you need to use OpenID, Facebook, MySpace, Twitter, Linkedin, etc.
  ## register with janrain.com, write your domain:api_key in private/janrain.key
  from gluon.contrib.login_methods.rpx_account import use_janrain
  use_janrain(auth, filename='private/janrain.key')
  
  #########################################################################
  ## Define your tables below (or better in another model file) for example
  ##
  ## >>> db.define_table('mytable',Field('myfield','string'))
  ##
  ## Fields can be 'string','text','password','integer','double','boolean'
  ##       'date','time','datetime','blob','upload', 'reference TABLENAME'
  ## There is an implicit 'id integer autoincrement' field
  ## Consult manual for more options, validators, etc.
  ##
  ## More API examples for controllers:
  ##
  ## >>> db.mytable.insert(myfield='value')
  ## >>> rows=db(db.mytable.myfield=='value').select(db.mytable.ALL)
  ## >>> for row in rows: print row.id, row.myfield
  #########################################################################
  
  ## after defining tables, uncomment below to enable auditing
  # auth.enable_record_versioning(db)
#+end_src
* Definitions and tips for each metadata field
The following sections show the definitions used to create the data inventory:
** Project Information and Data Owners
- The EML Project module is used to place the dataset that is being documented into its larger research context.
- The KNB definition can be found at these links: [[./eml-2.1.1/docs/eml-2.1.1/eml-project.html][EML 2.1.1 definition]] or [[https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-project.html][EML online]]
- The KNB says this is for 'Research context information for resources': The eml-project module describes the research context in which the dataset was created, including descriptions of over-all motivations and goals, funding, personnel, description of the study area etc. This is also the module to describe the design of the project: the scientific questions being asked, the architecture of the design, etc. 
- Morpho says: this metadata element is to recognise that data may be collected as part of a larger research program  (umbrella research project).  For example a large NSF grant may provide funds for several investigators to collect data at various locations.
- The new KNB metacat skin uses the title "Parent Project Information".
- Please also note the [[https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-project.html#relatedProject]['relatedProject']] tag which LTERN is not currently using.  This 'is a recursive link to another project. This allows projects to be nested under one another for the case where one project spawns another.'

*** COMMENT project code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  
  #### projects
  
  db.define_table(
      'project',
#+end_src
*** eml/dataset/project/title
The suggested structure is either:
- the [Plot Network Name] (ie 'Victorian Alpine Plot Network') , or 
- a label that describes the [geographic coverage], [data type] and links to the [people] or [organisations] who own the data (ie Australian Tundra Experiment : Long Term Ecological Research Network (LTERN) - Victorian Alpine Plot Network).
- The KNB definition can be found at these links: [[./eml-2.1.1/docs/eml-2.1.1/eml-project.html#title][EML 2.1.1 definition]] or [[https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-project.html#title][EML online]]
- If the data has been collected as part of a larger umbrella research project this is the title. Suggested structure is: [Generic Project Name] OR [geographic coverage] [data type].  Otherwise it is the same as the dataset title.
**** COMMENT project/title/code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  Field('title', 'string',
  comment= XML(T('The project places the data into its larger research context.  %s',
  A('More', _href=XML(URL('static','index.html',  anchor='sec-5-1-1', scheme=True, host=True)))))
  ),
#+end_src
*** eml/dataset/project/personnel
In EML the Data Owner is able to be shown at various points on the xpath.  
LTERN uses the dataset/project/personnel/individualName or dataset/project/personnel/organizationName with  dataset/project/personnel/role == 'Data Owner' 
- A data owner can be a person, an organisational role or an organisation who has a statutory and operational authority over data.
- Also , organisational roles or organisations , such as the originator
- The KNB definition can be found at these links: [[./eml-2.1.1/docs/eml-2.1.1/eml-project.html#personnel][EML 2.1.1 definition]] or [[https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-project.html#personnel][EML online]]

**** COMMENT personnel-code
# cut XML(URL('static','eml-2.1.1/docs/eml-2.1.1/eml-project.html',  anchor='personnel', scheme=True, host=True)))
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  Field('personnel_data_owner','string', 
  comment= XML(T('This is the data owner (or project originator). It is a compulsory field.  %s',
  A('More', _href=XML(URL('static','index.html',  anchor='sec-5-1-2', scheme=True, host=True)))))
  ),
  Field('personnel_owner_organisationname','string', 
  comment= XML(T('This is the data owner organisation. %s',
  A('More', _href=XML(URL('static','index.html',  anchor='sec-5-1-2', scheme=True, host=True)))))
  ),
#+end_src
**** personnel general
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  Field('personnel','string', 
  comment= XML(T('This is for key people etc that are not the owner. %s',
  A('More', _href=XML(URL('static','index.html',  anchor='sec-5-1-2', scheme=True, host=True)))))
  ),
#+end_src
*** funding-code
- Significant funding sources under which the data has been collected over the lifespan of the project
**** COMMENT code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('funding', 'text',
      comment= XML(T('Significant funding sources under which the data has been collected over the lifespan of the project. %s',
      A('More', _href=XML(URL('static','index.html',  anchor='sec-5-1-3', scheme=True, host=True)))))
      ),

#+end_src
*** eml/dataset/project/abstract
The following information is used to create the LTERN Data Portal record for each Data Package:
Complete this section of the contextual metadata last. In that way you can collate the key facts from the rest of the metadata elements – the ‘quick touch’ approach.
We use the informative abstract method. The eml/dataset/project/abstract should be a descriptive of the umbrella project, not the dataset.
- Briefly outline the relevant project or study including the Plot Network name and describe the contents of the data package. 
- Include geographic location, the primary objectives of the study, what data was collected (species or phenomena), the year range the data was collected in, and collection frequency if applicable.
- Describe methodology techniques or approaches only to the degree necessary for comprehension – don’t go into any detail.
- Cite references and/or links to any publications that are related to the data package.
- Single paragraph. 200-250 words.
- Use active voice and past tense.
- Use short complete sentences.
- Express terms in both their abbreviated and spelled out form for search retrieval purposes.
- The KNB definition can be found at these links: [[./eml-2.1.1/docs/eml-2.1.1/eml-project.html#abstract][EML 2.1.1 definition]] or [[https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-project.html#abstract][EML online]]
**** COMMENT abstract-code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('project_abstract', 'text',
      comment= XML(T('Descriptive abstract that summarizes information about the umbrella project context of the specific project. %s',
      A('More', _href=XML(URL('static','index.html',  anchor='sec-5-1-3', scheme=True, host=True)))))
      ),
#+end_src
*** eml/dataset/project/studyAreaDescription
- The KNB definition can be found at these links: [[./eml-2.1.1/docs/eml-2.1.1/eml-project.html#studyAreaDescription][EML 2.1.1 definition]] or [[https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-project.html#studyAreaDescription][EML online]]
**** COMMENT comment studydesc
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('studyAreaDescription','string', 
      comment= XML(T('This can include descriptions of the geographic, temporal, and taxonomic coverage of the research location. %s', 
      A('More', _href=XML(URL('static','index.html', anchor='sec-5-1-4', scheme=True, host=True)))))
      ),
#+end_src

*** project established
- Commencement date of overarching research project as a specific date or year
**** COMMENT code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('project_established','date', 
      comment= XML(T('Commencement date of overarching research project as a specific date or year. %s', 
      A('More', _href=XML(URL('static','index.html', anchor='sec-5-1-4', scheme=True, host=True)))))
      ),
#+end_src
*** citation
- Citations relevant to the design of the overarching project
**** COMMENT code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('project_citation','text', 
      comment= XML(T('Citations relevant to the design of the overarching project. %s', 
      A('More', _href=XML(URL('static','index.html', anchor='sec-5-1-4', scheme=True, host=True)))))
      ),
#+end_src
*** relatedProject
- A recursive link to another project. This allows projects to be nested under one another

**** COMMENT code 
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('related_project','text', 
      comment= XML(T('A recursive link to another project. This allows projects to be nested under one another. %s', 
      A('More', _href=XML(URL('static','index.html', anchor='sec-2-1-5', scheme=True, host=True)), _target='new')))
      ),
#+end_src

*** COMMENT end
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      format = '%(title)s' 
      )
  
      db.project.personnel_data_owner.requires = IS_NOT_EMPTY()
#+end_src
*** COMMENT DEPRECATED dataset-setup-code
#+name:dataset-setup
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:dataset-setup####
  
  library(gdata)
  indir <- "~/Dropbox/projects/DataDocumentation/emldb"
  dir(indir)
  dat <-  read.xls(file.path(indir, "setup_emldb_crosswalks_master.xlsx"))
  str(dat)
  head(dat)
  table(dat$eml.table)
  
  # project
  
  tbl <- "project"
  psql <- paste(
    as.character(
      dat[which(dat$eml.table == tbl & dat$w2p_code !=""),"w2p_code"]
      ), sep = "", collapse = "\n"
    )
  psql <- gsub("&apos;", "'", psql)
  cat(psql)
  
  # dataset
  dat[which(dat$eml.table == tbl),1:3]
  tbl <- "dataset"
  psql <- paste(
    as.character(
      dat[which(dat$eml.table == tbl & dat$w2p_code !=""),"w2p_code"]
      ), sep = "", collapse = "\n"
    )
  psql <- gsub("&apos;", "'", psql)
  cat(psql)
#+end_src

** Dataset 
*** COMMENT dataset
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  #### ONE (project) TO MANY (dataset)
  
  db.define_table(
      'dataset',
      Field('project_id',db.project),
#+end_src
*** dataset shortname
A concise name, eg. vernal-data-1999.
The 'shortName' field provides a concise name that describes the resource that is being documented. It is the appropriate place to store a filename associated with other storage systems.

- https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-resource.html#shortName
- http://127.0.0.1:8181/data_inventory/static/eml-2.1.1/docs/eml-2.1.1/eml-resource.html#shortName


**** COMMENT TODO shortname
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('shortname','string', comment = XML(T('A concise name, eg. vernal-data-1999. %s.',
      A('More', _href=XML(URL('static','index.html',  anchor='sec-5-2-1', scheme=True, host=True)),  _target='new')))
      ),
#+end_src
*** dataset title
TODO
**** COMMENT TODO title
[Project name (optional sub-project name)] [:] 
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('title','string', comment = XML(T('Structure eg: project, data type, location, temporal tranches. %s',
      A('More', _href=XML(URL('static','index.html',  anchor='sec-5-2', scheme=True, host=True)))))
      ),
#+end_src
*** dataset creator
TODO
**** COMMENT TODO creator
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('creator','string', comment='The name of the person, organization, or position who created the data'),
#+end_src
*** dataset contact
TODO
**** COMMENT TODO contact
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('contact','string', comment = 'A contact name for general enquiries.  This field defaults to creator.'),
      Field('contact_email','string', comment = 'An email address for general enquiries.'),
#+end_src
*** dataset abstract
This should be brief but include:
- Study research question
- Specific hypothesis under study
- Endpoints or outcomes of interest
**** COMMENT TODO code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('abstract','text', comment = XML(T('A brief overview of the resource that is being documented. The abstract should include basic information that summarizes the study/data. %s', A('More', _href=XML(URL('static', 'index.html',  anchor='sec-5-2', scheme=True, host=True)))))),
#+end_src

*** dataset additional metadata links
TODO
**** COMMENT code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('additional_metadata' ,'string', comment="Any additional metadata such as folder path or URL links to related webpages."),
#+end_src
*** dataset recommended citation
A resource that describes a literature citation that one might find in a bibliography. If included, this represents the primary resource that is described in this eml document. 

- https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/./eml.html#citation 

**** code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('recommended_citation', 'text', comment="For example: 1. Creator (Publication Year): Title. Publisher. Identifier. 2. Creator (Publication Year): Title. Publisher. Date retrieved from website (URL). 3. Creator (Publication Year): Title. Publisher. Date received from data provider (name, role or organisation)."),
#+end_src
*** dataset methods/studyextent
- http://127.0.0.1:8181/data_inventory/static/eml-2.1.1/docs/eml-2.1.1/eml-methods.html#studyExtent
- Both specific to sampling area and frequency, spatial extent/resolution, temporal boundaries. eg CCD 2001.
**** COMMENT code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('studyextent' ,'text', comment="Both a specific sampling area and frequency (temporal boundaries, frequency of occurrence, spatial extent and spatial resolution)."),
#+end_src
*** dataset temporal coverage
**** COMMENT code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('temporalcoverage_daterange','string', comment = "A text description of the temporal range that events were observed on"),
      Field('temporalcoverage_begindate','date', comment="A begin date.  The dates that events were observed on."),
      Field('temporalcoverage_enddate','date', comment="A end date. The dates that events were observed on."),
#+end_src


*** dataset methods protocol
**** general EML methods module info
- http://127.0.0.1:8181/data_inventory/static/eml-2.1.1/docs/eml-2.1.1/eml-methods.html
- The eml-methods module describes the methods followed in the creation of the dataset, including description of field, laboratory and processing steps, sampling methods and units, quality control procedures. The eml-methods module is used to describe the actual procedures that are used in the creation or the subsequent processing of a dataset. Likewise, eml-methods is used to describe processes that have been used to define / improve the quality of a data file, or to identify potential problems with the data file. Note that the eml-protocol module is intended to be used to document a prescribed procedure, whereas the eml-method module is used to describe procedures that were actually performed. The distinction is that the use of the term "protocol" is used in the "prescriptive" sense, and the term "method" is used in the "descriptive" sense. This distinction allows managers to build a protocol library of well-known, established protocols (procedures), but also document what procedure was truly performed in relation to the established protocol. The method may have diverged from the protocol purposefully, or perhaps incidentally, but the procedural lineage is still preserved and understandable. 

- eml-methods is descriptive (often written in the declarative mood: "I took five subsamples...") whereas eml-protocol is prescriptive (often written in the imperative mood: "Take five subsamples...").

**** Recommendations for protocol
The protocol information can be captured in two places of EML
- http://127.0.0.1:8181/data_inventory/static/eml-2.1.1/docs/eml-2.1.1/eml-protocol.html
- http://127.0.0.1:8181/data_inventory/static/eml-2.1.1/docs/eml-2.1.1/eml-methods.html#protocol
- In many cases the simpler option is to use the methods module protocol section which is designed to capture 'reference a protocol resource or describe methods and identify the processes that have been used to define / improve the quality of a data file'.
- It can also be used to identify potential problems with the data collection protocol or study design, to be addressed in the methods steps.


After deciding where the protocol information will sit, then consider including the following information:
- Study type: select from a list like time-series, cross-sectional, spatial, exploratory, confirmatory
- Data sets collected/used 
- Analysis package (statistical package name and version)
- Study population (including inclusion and exclusion criteria)
- Exposure variables
- Outcome measures (outcome measures, including comparison group)
- Covariates (possible exposures, potential confounders or effect modifiers)

If possible also details of the sequence of planned analyses, with statistical methods to be employed and how each of the planned hypotheses will be tested. Include details of how potential confounding and bias will be dealt with. For example:

1. Descriptive analyses
2. Univariable analysis
3. Multivariable analysis, and
4. Interactions, subgroups, sensitivity analyses

*** dataset sampling description
- Similar to a description of sampling procedures found in the methods section of a journal article.
- http://127.0.0.1:8181/data_inventory/static/eml-2.1.1/docs/eml-2.1.1/eml-methods.html#samplingDescription
*** dataset method steps
- http://127.0.0.1:8181/data_inventory/static/eml-2.1.1/docs/eml-2.1.1/eml-methods.html#methodStep
- EACH method step to implement the measurement protocols and set up the study. Note that the method is used to describe procedures that were actually performed. The method may have diverged from the protocol purposefully, or perhaps incidentally, but the procedural lineage is still preserved and understandable.
- Include data cleaning strategy, and process for handling missing data.
- Include text descriptions of the procedures, relevant literature, software, instrumentation, source data and any quality control measures taken.

*** COMMENT dataset methods sampling, steps, protocol codes
**** COMMENT codes
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('methods_protocol' , 'text', comment = XML(T('The protocol field is used to either reference a protocol citation or describe the methods that were prescribed to define a study or dataset. Note that the protocol is intended to be used to document a prescribed procedure which may or may not have been performed (see Method Steps). %s', A('More', _href=XML(URL('static', 'index.html',  anchor='sec-5-2-9', scheme=True, host=True)))))),
      Field('sampling_desc' ,'text', comment = XML(T('Similar to a description of sampling procedures found in the methods section of a journal article. %s', A('More', _href=XML(URL('static', 'index.html',  anchor='sec-5-2-10', scheme=True, host=True)))))),
      Field('method_steps','text', comment=XML(T('EACH method step to implement the measurement protocols and set up the study. Note that the method is used to describe procedures that were actually performed. The method may have diverged from the protocol purposefully, or perhaps incidentally, but the procedural lineage is still preserved and understandable. %s', A('More', _href=XML(URL('static', 'index.html',  anchor='sec-5-2-11', scheme=True, host=True)))))),
#+end_src

*** COMMENT dataset assoc

#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('associated_party','text', comment = XML(T('A person, organisational role or organisation who has had an important role in the creation or maintenance of the data (i.e. parties who grant access to survey sites as landholder or land manager, or may have provided funding for the surveys). %s.',
    A('More', _href=XML(URL('static','index.html',  anchor='sec-5-2', scheme=True, host=True)))))
      ),
#+end_src
*** TODO dataset Geographic
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no

      Field('geographicdescription','string',
      comment = XML(T('A general description of the geographic area in which the data were collected. This can be a simple place name (e.g. Kakadu National Park). %s',
      A('More', _href=XML(URL('static','index.html',  anchor='sec-5-2', scheme=True, host=True)))))     
      ),
#+end_src
*** TODO dataset bounding coords
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('boundingcoordinates','string',
      comment = XML(T('bounding coordinates in order N, S, E, W (Optionally also add altitudeMinimum, altitudeMax). %s',
      A('More', _href=XML(URL('static','index.html',  anchor='sec-5-2', scheme=True, host=True)))))     
      ),
#+end_src
*** TODO dataset taxo cov
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('taxonomic_coverage','string', comment="List of scientific names."),
#+end_src
*** additional info
Any information that is not characterised well by EML metadata. Example is a group id for grouping datasets apart from EML-project (such as a funding stream, or a particular documentation such as provision agreement).

- EML Standard: https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-resource.html#additionalInfo
- Local Link: http://127.0.0.1:8181/data_inventory/static/eml-2.1.1/docs/eml-2.1.1/eml-resource.html#additionalInfo

**** COMMENT codes
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('additionalinfo','string', comment = XML(T('Any information that is not characterised well by EML metadata. Example is a group id for grouping datasets apart from EML-project (such as a funding stream, or a particular documentation such as provision agreement). %s.',
    A('More', _href=XML(URL('static','index.html',  anchor='sec-5-2-15', scheme=True, host=True)),  _target='new')))
      ),
#+end_src
*** TODO dataset alternateidentifier
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('alternateidentifier','string',
      comment = XML(T('Additional identifier that is used to label this dataset. This might be a DOI, or other persistent URL. %s.',
      A('More', _href=XML(URL('static','index.html',  anchor='sec-5-2', scheme=True, host=True)))))     
      ),
#+end_src
*** TODO dataset pubdate
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('pubdate','date'),
#+end_src
*** TODO access rules 
for instance are ethics protocol details required?

#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('access_rules','text', comment = "The eml-access module describes the level of access that is to be allowed or denied to a resource for a particular user or group of users"),
#+end_src
*** TODO distribution method
**** ideas
new field = dataset/distribution_methods
The methods of distribution used for others to access the software, data, and documentation.

Distribution strategies include:
- publication of journal papers, reports, conference presentations, blogging, twitter and news media
- data deposit in a portal or archive
- code publication online to open-source code sharing sites, or made available on request.

This deals with IP and copyright, so maybe just have well been in the intellectual_right module?
**** code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('distribution_methods','text', comment = "The methods of distribution used for others to access the software, data, and documentation."),
#+end_src
*** TODO metadataprovider
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('metadataprovider','string', comment = 'The name of the person who produced the metadata.'),
#+end_src
*** TODO provision status
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('provision_status','string', comment = 'The status of this data provision (Identified, Requested or Provided).'),
#+end_src
*** TODO provision status
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('provision_date','date', comment = 'The date provided.'),
#+end_src

*** TODO request notes
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('request_notes','text', comment = 'Any relevant information regarding this data provision request.'),
#+end_src
*** TODO request date
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('request_date','date', comment = 'Date provision requested.'),
#+end_src

*** COMMENT TODO end
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      format = '%(shortname)s'
      )
  
  db.dataset.contact_email.requires = [IS_EMAIL()]
  db.dataset.creator.requires = [IS_NOT_EMPTY()]
  db.dataset.provision_status.requires = IS_IN_SET(['','Identified', 'Requested', 'Provided', 'QC', 'Published'])      
  # db.dataset.metadataprovider.requires = [IS_EMAIL(), IS_NOT_IN_DB(db, 'dataset.metadataprovider')]
     
#+end_src


*** COMMENT old dataset
#+begin_src markdown :tangle no :exports reports :eval no :padline no
  
  #### ONE (project) TO MANY (dataset)
  
  db.define_table(
      'dataset',
      Field('project_id',db.project),
      Field('title','string'),
      Field('creator', 'string'),
      Field('contact','string'),
      Field('intellectualright','string'),
      Field('pubdate','date'),
      Field('geographicdescription','string'),
      Field('temporalcoverage','string'),
      Field('metadataprovider','string'),
      format = '%(title)s'
      )

  db.dataset.metadataprovider.requires = [IS_EMAIL(), IS_NOT_IN_DB(db, 'dataset.metadataprovider')]

#+end_src
*** COMMENT DEPRECATED, USE Pubs table instead: dataset methods citation

#+begin_src markdown :tangle no :exports reports :eval no :padline no
      Field('methods_citation' ,'string', comment="The citation field allows to either reference a literature resource or enter structured literature information."),
#+end_src
** Data Entity (dataTable)
- The entity level is for data files, tables, shapefiles, photos etc.  
- I also use this for folders
- https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/#N10115
- https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-dataTable.html#numberOfRecords
**** COMMENT entity code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
    
#### ONE (dataset) TO MANY (entity)
  
db.define_table(
      'entity',
#+end_src
*** eml/dataset/entity/id
**** COMMENT entity code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('dataset_id',db.dataset),
#+end_src
*** eml/dataset/entity/name
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('entityname','string', comment = "The file name, name of database table, etc. It should identify the entity in the dataset. Example: SpeciesAbundance1996.csv", requires = IS_NOT_EMPTY()),
#+end_src
*** eml/dataset/entity/description
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('entitydescription', 'string', comment = "Text generally describing the entity, its type, and relevant information about the data in the entity. Example: Species abundance data for 1996 at the VCR LTER site"),
#+end_src

*** eml/dataset/entity/physical/distribution
- The PhysicalDistribution contains the information required for retrieving the resource.
- The KNB definition can be found at these links: [[./eml-2.1.1/docs/eml-2.1.1/eml-physical.html#distribution][EML 2.1.1 definition]] or [[https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-physical.html#distribution][EML online]]
**** COMMENT entity code
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('physical_distribution', 'string',
comment= XML(T('Information required for retrieving the resource. %s',    
      A('More', _href=XML(URL('static','index.html',  anchor='sec-5-3-4', scheme=True, host=True)))))
      ),
      Field('physical_distribution_additionalinfo', 'text',
comment= XML(T('Additional Information about the storage of the resource, including backup regime. %s',    
      A('More', _href=XML(URL('static','index.html',  anchor='sec-5-3-4', scheme=True, host=True)))))
      ),
#+end_src

*** TODO temporal cov
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('entity_temporalcoverage_daterange','string', comment = "A text description of the temporal range that events were observed on"),
#+end_src

*** eml/dataset/entity methods
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('entity_methods', 'text', comment = "Information on the specific methods used to collect information in this entity."),
#+end_src
*** COMMENT eml/dataset/entity num recs

#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
      Field('numberOfRecords', 'integer', comment = 'The number of rows in a table.'),
      format = '%(entityname)s'
      )


#+end_src
** COMMENT attr
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  
  #### ONE (entity) TO MANY (attributes/variables)
  
  db.define_table(
      'attr',
      Field('entity_id',db.entity),
      Field('variable_name', 'string', comment = 'The name of the variable'),
      Field('variable_definition', 'string', comment = 'Definition of the variable.'),
      Field('measurement_scales', 'string', comment = 'One of nominal, ordinal, interval, ratio or datetime', requires = IS_IN_SET(['nominal', 'ordinal', 'interval', 'ratio', 'datetime'])),
      Field('units', 'string', comment = 'Standard Unit of Measurement'),
      Field('value_labels', 'string', comment = 'Labels for levels of a factor.  For example a=bud, b=flower, c=fruiting')      
      )
#+end_src
** COMMENT accessdataset 
An optional access tree at this location controls access to the entire metadata document. If this access element is omitted from the document, then the package submitter should be given full access to the package but all other users should be denied all access. 
https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml.html#access
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  
#### accessdatasets
  
db.define_table(
    'accessdataset',
    Field('name','string',
comment= XML(T('A person or group. Keep this to a short (two or three word) title as it is used to specify access requests in the acessrequest table). %s',    
    A('More', _href=XML(URL('static','index.html',  anchor='sec-5-3-4', scheme=True, host=True)))))
    ),
    Field('email'),
    Field('bio', 'string', comment = "A short description of this person/group."),
    format = '%(name)s'
    )
db.accessdataset.name.requires = IS_NOT_EMPTY()
db.accessdataset.email.requires = [IS_EMAIL(), IS_NOT_EMPTY()]
  
#+end_src
** COMMENT accessor
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  
  #### MANY (accessors) TO MANY (accessdataset members)
  
  db.define_table(
      'accessor',
      Field('accessdataset_id',db.accessdataset),
      Field('name'),
      Field('email'),
      Field('role', 'string', comment = "The role that this person will have in the project, specifically in relation to the data."),
      format = '%(name)s'
      )
  db.accessor.email.requires = [IS_EMAIL()]
  # , IS_NOT_IN_DB(db, 'accessor.email')]
         
#+end_src
** COMMENT access request
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  
  #### MANY (datasets) TO MANY (accessors)
  
  db.define_table(
      'accessrequest',
      Field('dataset_id',db.dataset),
      Field('accessdataset_id',db.accessdataset),
      Field('title', 'string', comment = "A short (two or three word) title of the project for which the data are to be used"),
      Field('description', 'text', comment = "A description of the project for which the data are to be used. Include description of any ethics committee approvals and the intended publication strategy."),
      Field('begin_date', 'date', comment = "Access granted on this date"),
      Field('end_date', 'date', comment = "Access revoked on this date"),
      format = '%(title)s %(accessdataset_id)s -> %(dataset_id)s'
      )

#+end_src
** COMMENT keywords
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  
  #### MANY (keywords) TO one (dataset)
  
  db.define_table(
      'keyword',
      Field('dataset_id',db.dataset),
      Field('thesaurus', 'string', comment = 'source of authoritative definitions'),
      Field('keyword', 'string', requires=IS_IN_DB(db, 'thesaurus_ltern.keyword'))
      )
    
#+end_src

** COMMENT intellectualright
http://creativecommons.org/licenses/
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
    
#### ONE (intellectualright) TO one (dataset)
db.define_table(
    'intellectualright',
    Field('dataset_id',db.dataset),
    Field('data_owner', 'string', comment = 'The person or organisation with authority to grant permissions to access data.'),
    Field('data_owner_contact', 'string', comment = 'Optional.'),
    Field('licencee', comment = 'Optional.'),    
    Field('licence_code', 'string', comment = XML(T("The licence to allow others to copy, distribute or display work and derivative works based upon it and define the way credit will be attributed. Common licences are 'CCBY', 'CCBYSA',  'CCBYND', 'CCBYNC', 'CCBYNCSA', 'CCBYNCND' or 'other'. For more information see http://creativecommons.org/licenses/. %s",     A('More', _href=XML(URL('static','index.html',  anchor='sec-5-2', scheme=True, host=True)))))
    ),
    Field('licence_text', 'string', comment = 'The name of the licence.'),
    Field('special_conditions', 'text', comment = 'Any restrictions to be placed on the access or use, especially the timeframe if this is limited.'),
    Field('path_to_licence', 'string', comment = 'Optional.')
    )
    
db.intellectualright.licence_code.requires = IS_IN_SET(['CCBY', 'CCBYSA',  'CCBYND', 'CCBYNC', 'CCBYNCSA', 'CCBYNCND', 'other'])    

#+end_src
** COMMENT checklists
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no

#### ONE (checklist) TO one (dataset)
db.define_table(
    'checklist',
    Field('dataset_id',db.dataset),
Field('checked_by','string'),
Field('check_date','date'),
Field('notes_comments','text'),
Field('data_package_title_check','boolean'),
Field('data_set_citation_check','boolean'),
Field('data_package_owner_check','boolean'),
Field('data_package_owner_check_individual_name','boolean'),
Field('data_package_owner_check_position_role','boolean'),
Field('data_package_owner_check_organization','boolean'),
Field('data_package_owner_check_address','boolean'),
Field('data_package_owner_check_phone','boolean'),
Field('data_package_owner_check_email_address','boolean'),
Field('associated_parties','boolean'),
Field('associated_parties_individual_name','boolean'),
Field('associated_parties_position','boolean'),
Field('associated_parties_organization','boolean'),
Field('associated_parties_physical_address','boolean'),
Field('associated_parties_phone','boolean'),
Field('associated_parties_email_address','boolean'),
Field('abstract','boolean'),
Field('keywords_and_subject_categories','boolean'),
Field('gcmd_science_keywords','boolean'),
Field('anzsrc_for_codes','boolean'),
Field('ltern_monitoring_themes','boolean'),
Field('keywords_free_text','boolean'),
Field('geographic_coverage','boolean'),
Field('geographic_description','boolean'),
Field('bounding_coordinates','boolean'),
Field('temporal_coverage','boolean'),
Field('contacts_individual_names','boolean'),
Field('contacts_positions','boolean'),
Field('contacts_organizations','boolean'),
Field('contacts_addresses','boolean'),
Field('contacts_phone','boolean'),
Field('contacts_email_addresses','boolean'),
Field('methods_and_sampling_information','boolean'),
Field('method_step_titles','boolean'),
Field('method_step_description','boolean'),
Field('instrumentation_details','boolean'),
Field('sampling_area_and_frequency','boolean'),
Field('sampling_description','boolean'),
Field('research_project_title','boolean'),
Field('research_project_funding_sources','boolean'),
Field('research_project_personnel_information','boolean'),
Field('research_project_individual_name','boolean'),
Field('research_project_position_role','boolean'),
Field('research_project_organization','boolean'),
Field('research_project_address','boolean'),
Field('research_project_phone','boolean'),
Field('research_project_email_address','boolean'),
Field('research_project_role','boolean'),
Field('additional_metadata','boolean'),

Field('access_control','boolean'),

Field('usage_rights','boolean'),
Field('special_conditions','boolean'),
Field('entity_metadata','boolean'),
Field('homepage_content','boolean'),
Field('eml_homepage_links','boolean'),
Field('can_the_plot_network_or_data_package_be_filtered_in_the_search_bar_of_the_portal','boolean'),
Field('draft_publication_checklist_passed','boolean'),
Field('metacat_publication_checklist_check_public_or_mediated_access','boolean'),
Field('metacat_publication_checklist_add_publication_date_to_data_inventory','boolean'),
Field('metacat_publication_checklist_passed','boolean'),
Field('reporting_checklist_licenced','boolean'),
Field('reporting_checklist_described_with_metadata_','boolean'),
Field('reporting_checklist_doi_minted','boolean'),
Field('reporting_checklist_metadata_feed_to_tddp_and_rda','boolean'),
Field('reporting_checklist_passed','boolean')
    )
    
db.checklist.checked_by.requires = IS_IN_SET(['Claire', 'Karl'])
db.checklist.check_date.requires = IS_NOT_EMPTY()
  
#+end_src

** COMMENT errors and addenda
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no

#### ONE (errors) TO one (dataset)
db.define_table(
    'error',
    Field('dataset_id',db.dataset),
Field('logged_by','string'),
Field('date_logged','date'),
Field('date_actioned','date'),
Field('error','text'),
Field('addenda','text')
    )
    
db.error.logged_by.requires = IS_NOT_EMPTY()
db.error.date_logged.requires = IS_NOT_EMPTY()    
#+end_src
** COMMENT publications
*** biblio table
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  
#### ONE (biblio) TO one (entity)
db.define_table(
'publication',
Field('dataset_id',db.dataset),
Field('bibtex_key', 'string', requires = IS_NOT_EMPTY(),  comment = "For eg from mendeley, use ctrl-k or copy as.  it will be like \cite{xyz}.  COMPULSORY."),
Field('publication_type','string', requires = IS_IN_SET(['Papers', 'Conference Presentations', 'Reports', 'Policy Briefs', 'Data Packages', 'Software', 'Media'])),
Field('publication_status','string', requires = IS_IN_SET(['Draft', 'Submitted', 'Revision', 'Published (peer-reviewed)', 'Published (not peer-reviewed)', 'Self-published (not peer-reviewed)'])),
Field('publication_status_deadline','date', comment = 'This is the date that the current phase will finish and the next phase of publication starts'),
Field('title','string'),
Field('citation', 'string', comment = 'At a minimum author-date-journal, perhaps DOI?'),
Field('key_results', 'text', comment = 'Include both effect estimates and uncertainty'),
Field('background_to_study', 'string', comment = ''),
Field('research_question', 'string', comment = ''),
Field('study_extent', 'string', comment = ''),
Field('outcomes','string', comment = ''),
Field('exposures','string', comment = ''),
Field('covariates','string', comment = 'Include covariates, effect modifiers, confounders and subgroups'),
Field('method_protocol', 'text', comment = ''),
Field('general_comments', 'text', comment = ''),
Field('publication_description', 'string'),
Field('google_pubid','string', comment = 'The unique ID used by google scholar'),
Field('journal','string'),
Field('year_published','integer'),
Field('impact_factor','double'),
Field('date_impact_factor_checked','date'),
Field('google_scholar_cites','integer'),
Field('date_gs_cites_checked','date'),
Field('web_of_science_cites','integer'),
Field('date_wos_cites_checked','date'),
Field('contribution','text'),
Field('thesis_section','string'),
Field('thesis_context_statement','text'),
Field('thesis_publication_status','string')
)
     
#+end_src
*** COMMENT R bibtex-code
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:R bibtex ####
  # if using postgres
  library(rpostgrestools)
  if(!exists("ch")) ch <- connect2postgres2("data_inventory_hanigan_dev4")
  # if using sqlite
    ## library(RSQLite)
    ## drv <- dbDriver("SQLite")
    ## ch <- dbConnect(drv, dbname = "~/tools/web2py/applications/data_inventory/databases/storage.sqlite")
  
  # get the literature resource ids
  citn  <- dbGetQuery(ch,
  "select id, bibtex_key
  from publication
  where bibtex_key not like '%Hanigan2008a%' 
  ")
  citn
  citn$bibid  <- gsub("\\}", "", gsub("\\\\cite\\{", "", citn[,2]))
  citn
  
  # get the bibtex info
  library(knitcitations)
  library(bibtex)
  cleanbib()
  cite_options(citation_format = "pandoc", check.entries=FALSE) 
  #if(!exists("bib"))c
    bib <- read.bibtex("~/references/library.bib")
  
  # update the db with the bibtex info
  citn
  for(citn_i in 1:nrow(citn)){
  #citn_i
  ct <- bib[[citn[citn_i, 3]]]
  #print(  ct[[1]]$year)
  
  
  titl  <- gsub("\\}", "", gsub("\\{", "", ct[[1]]$title))
  id  <-  citn[citn_i, 1]
  dbSendQuery(ch,
  #            cat(
    sprintf("update publication set title = '%s', year_published = %s where id = %s",  gsub("'", '"', titl), ct[[1]]$year, id)
              )
  }
#+end_src

#+RESULTS:

*** COMMENT R bibtex authors -code
#+name:R bibtex
#+begin_src R :session *R* :tangle no :exports none :eval no
  
  # author approvals, careful not to re-run!
  #for(citn_i in 1:nrow(citn)){
  citn[,3]
  citn_i = 23
  ct <- bib[[citn[citn_i, 3]]]
  #print(  ct[[1]]$year)
  ct
  
  titl  <- gsub("\\}", "", gsub("\\{", "", ct[[1]]$title))
  id  <-  citn[citn_i, 1]
  ## dbSendQuery(ch,
  ## #             cat(
  ##   sprintf("update publication set title = '%s', year_published = %s where id = %s",  titl, ct[[1]]$year, id)
  ##             )
  
  auths <- ct[[1]]$author
  auths <- lapply(auths, paste)
  auths[[length(auths) + 1]] <- "Copyright"
  auths[[length(auths) + 1]] <- "Ethics Approval"
  auths <- gsub("'", "", auths)
  for(name_i in auths){
  #  name_i = auths[1]
  dbSendQuery(ch,
  #            cat(
  sprintf("insert into authorship_approval(publication_id, name) values (%s, '%s')\n", id, name_i)
              )
              
  }
  
  #}
#+end_src
*** COMMENT summarise approvals-code
#+name:summarise approvals
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:summarise approvals ####
  library(rpostgrestools)
  if(!exists("ch")) ch <- connect2postgres2("data_inventory_hanigan_dev4")
  id <- 42
  dat <- dbGetQuery(ch,
  #            cat(
  sprintf("select name, email,  notes, date_approval_given as date_approved from authorship_approval where publication_id = %s order by id", id)
              )
  str(dat)
  library(xtable)
  tabcode <- xtable(dat)
  align(tabcode) <-  c( 'l', 'p{1in}','p{1.5in}', '>{\\centering}p{2in}', 'p{.7in}' )
  print(tabcode, include.rownames = F)
#+end_src

*** COMMENT DEPRECATED R bibtex, get formated citation from bib
#+name:R bibtex
#+begin_src R :session *R* :tangle no :exports none :eval no
  setwd("~/tools/web2py/applications/data_inventory/static")
  
  
  ct[[1]]
  #$author
  txt_hd <- "---
  title: Untitled
  output: html_document
  csl: chicago-author-date.csl
  bibliography: references.bib
  ---\n\n
  
  ```{r, echo = F, results = 'hide'}
  # get the bibtex info
  library(knitcitations)
  library(bibtex)
  cleanbib()
  cite_options(citation_format = 'pandoc', check.entries=FALSE) 
  if(!exists('bib')) bib <- read.bibtex('~/references/library.bib')
    
  ```
  \n
  "
  
  txt <- ""
  for(citn_i in 1:nrow(citn)){
  #citn
  
  txt <- paste(txt,
               sprintf(
  "`r citet(bib[['%s']])`"
  ,
  citn[citn_i, "bibid"]
  )
  )
  }
  cat(txt)
  txt_bs <- "
  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file='references.bib')
  ```
  "
  
  sink("bib.Rmd")
  cat(txt_hd)
  cat(txt)
  cat(txt_bs)
  sink()
  rmarkdown::render("bib.Rmd", "html_document")
  rm(bib)
  
  
  
  
  
  
  
  
#+end_src


** COMMENT coauthors approvals (and journal copyright controls)
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no

#### many approval_to_share TO one paper
db.define_table(
    'authorship_approval',
    Field('publication_id',db.publication),
Field('name','string'),
Field('email','string'),
Field('organisation', 'string'),
Field('date_request_sent','date'),
Field('date_approval_given','date'),
Field('times_contacted','integer'),
Field('notes','text'),
Field('extra_details', 'text')
    )
     
#+end_src
* COMMENT Design User Interface
** Defaults for controllers/default.py
- Following the example in the documentation 

#+name:controllers/default.py
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/controllers/default.py :exports none :eval no :padline no
  # -*- coding: utf-8 -*-
  # this file is released under public domain and you can use without limitations
  
  #########################################################################
  ## This is a sample controller
  ## - index is the default action of any application
  ## - user is required for authentication and authorization
  ## - download is for downloading files uploaded in the db (does streaming)
  ## - call exposes all registered services (none by default)
  #########################################################################
  
  
  def index():
      """
      example action using the internationalization operator T and flash
      rendered by views/default/index.html or views/generic.html
  
      if you need a simple wiki simply replace the two lines below with:
      return auth.wiki()
      """
      response.flash = T("Welcome to the data inventory!")
      return dict(message=T('This is a data inventory for ecological data collections'))
   
  
  def user():
      """
      exposes:
      http://..../[app]/default/user/login
      http://..../[app]/default/user/logout
      http://..../[app]/default/user/register
      http://..../[app]/default/user/profile
      http://..../[app]/default/user/retrieve_password
      http://..../[app]/default/user/change_password
      http://..../[app]/default/user/manage_users (requires membership in
      use @auth.requires_login()
          @auth.requires_membership('group name')
          @auth.requires_permission('read','table name',record_id)
      to decorate functions that need access control
      """
      return dict(form=auth())
  
  @cache.action()
  def download():
      """
      allows downloading of uploaded files
      http://..../[app]/default/download/[filename]
      """
      return response.download(request, db)
  
  
  def call():
      """
      exposes services. for example:
      http://..../[app]/default/call/jsonrpc
      decorate with @services.jsonrpc the functions to expose
      supports xml, json, xmlrpc, jsonrpc, amfrpc, rss, csv
      """
      return service()
  
  
  @auth.requires_signature()
  def data():
      """
      http://..../[app]/default/data/tables
      http://..../[app]/default/data/create/[table]
      http://..../[app]/default/data/read/[table]/[id]
      http://..../[app]/default/data/update/[table]/[id]
      http://..../[app]/default/data/delete/[table]/[id]
      http://..../[app]/default/data/select/[table]
      http://..../[app]/default/data/search/[table]
      but URLs must be signed, i.e. linked with
        A('table',_href=URL('data/tables',user_signature=True))
      or with the signed load operator
        LOAD('default','data.load',args='tables',ajax=True,user_signature=True)
      """
      return dict(form=crud())

#+end_src
** Top Menu 
- From example 30 we get a simple user registration form. SQLFORM takes a table and returns the corresponding entry form with validators, etc. 
- The response.menu on top is just a variable used by the layout to make the navigation menu for all functions in this controller.
#+name:top menu and register a person
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/controllers/forms.py :exports reports :eval no
  response.menu = [['Inventory Home', False, URL('data_inventory','default','index')],
                   ['Manage Projects', False, URL('manage_projects')],
                   ['Manage Datasets', False, URL('manage_datasets')],
                   ['Manage Accessors or Groups', False, URL('manage_accessors_or_groups')],
                   ['Set Access to a Dataset', False, URL('access_dataset')],
                   ['Documentation', False, XML(URL('static','index.html', scheme=True, host=True))]]
#+end_src
** COMMENT Deprecated registor accessor or group-code
#+name:registor accessor or group
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:registor accessor or group####
# from response.menu                 ['Register Accessor or Group', False, URL('register_accessor_or_group')],


def register_accessor_or_group():
    # create an insert form from the table
    form = SQLFORM(db.accessdataset).process()

    # if form correct perform the insert
    if form.accepted:
        response.flash = 'new record inserted'

    # and get a list of all persons
    records = SQLTABLE(db().select(db.accessdataset.ALL),headers='fieldname:capitalize')

    return dict(form=form, records=records)

#+end_src

** Access a dataset
- Modified Example 32 from a sophisticated form that  checks the accessor  and the dataset are in the database and updates the corresponding record or inserts a new access. This version just adds another access record for each request.
- removed           
- title=form.vars.title
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/controllers/forms.py :exports reports :eval no :padline no
  def access_dataset():
      form = SQLFORM.factory(
          Field('accessdataset_id',requires=IS_IN_DB(db,db.accessdataset.id,'%(name)s')),
          Field('dataset_id',requires=IS_IN_DB(db,db.dataset.id,'%(shortname)s')),
          Field('title','string',requires=IS_NOT_EMPTY()),
          Field('description','text',requires=IS_NOT_EMPTY()),
          Field('begin_date','date'),
          Field('end_date','date')).process()

      
      if form.accepted:
          # get previous access for same dataset
          access = db((db.accessrequest.accessdataset_id == form.vars.accessdataset_id)&
              (db.accessrequest.dataset_id==form.vars.dataset_id)).select().first()
  
          db.accessrequest.insert(accessdataset_id=form.vars.accessdataset_id,
                           dataset_id=form.vars.dataset_id,
                           title=form.vars.title,
                           description=form.vars.description,
                           begin_date =form.vars.begin_date,
                           end_date   =form.vars.end_date
                           )
  
          response.flash = 'dataset accessed!'
      elif form.errors:
          response.flash = 'invalid values in form!'
  
      
      # now get a list of all accesses
      accessing = (db.accessdataset.id==db.accessrequest.accessdataset_id)&(db.dataset.id==db.accessrequest.dataset_id)
      records = SQLTABLE(db(accessing).select(),headers='fieldname:capitalize')
      return dict(form=form, records=records)
  
  
#+end_src
** COMMENT DEPRECATED Access a dataset
- Example 32 is a rather sophisticated buy form. It checks that the buyer and the product are in the database and updates the corresponding record or inserts a new purchase. It also does a JOIN to list all purchases. 
# controllers/forms.py
#+begin_src R :session *R* :tangle no :exports reports :eval no :padline no
  def access_dataset():
      form = SQLFORM.factory(
          Field('accessor_id',requires=IS_IN_DB(db,db.accessor.id,'%(email)s')),
          Field('dataset_id',requires=IS_IN_DB(db,db.dataset.id,'%(title)s')),
          Field('title','string',requires=IS_NOT_EMPTY())).process()
      if form.accepted:
          # get previous access for same dataset
          access = db((db.access.accessor_id == form.vars.accessor_id)&
              (db.access.dataset_id==form.vars.dataset_id)).select().first()
  
          if access:
              # if list contains a record, update that record
              access.update_record(
                  title = form.vars.title)
          else:
              # self insert a new record in table
              db.access.insert(accessor_id=form.vars.accessor_id,
                               dataset_id=form.vars.dataset_id,
                               title=form.vars.title)
          response.flash = 'dataset accessed!'
      elif form.errors:
          response.flash = 'invalid values in form!'
  
      
      # now get a list of all purchases
      accessing = (db.accessor.id==db.access.accessor_id)&(db.dataset.id==db.access.dataset_id)
      records = SQLTABLE(db(accessing).select(),headers='fieldname:capitalize')
      return dict(form=form, records=records)
  
#+end_src
** COMMENT  Register access view
DEPRECATED [ {{=A('delete accesses',_href=URL('delete_accessed'))}} ]
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/views/forms/access_dataset.html :exports reports :eval no :padline no
  
  {{extend 'layout.html'}}
  <h1>Access form</h1>
    {{=form}}
    <h2>Current access (SQL JOIN!)</h2>
  <p>{{=records}}</p>
  
  
#+end_src

** Main form ties it all together (manage projects)
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/controllers/forms.py :exports reports :eval no :padline no
  
  def manage_projects():
      grid = SQLFORM.smartgrid(db.project,linked_tables=['dataset', 'entity', 'publication', 'intellectualright', 'attr','accessrequest', 'authorship_approval'
                                                        ],
                               fields = [db.project.title, db.project.personnel_data_owner,
                                         db.dataset.shortname,


                                         db.dataset.additional_metadata,                                          

                                         db.entity.entityname, db.entity.entitydescription, db.entity.physical_distribution,
                                         db.attr.variable_name, db.attr.variable_definition,
                                         db.accessrequest.accessdataset_id, 
                                         db.accessrequest.dataset_id,
                                         db.accessrequest.title, 

                                         db.intellectualright.licence_code,
                                         db.publication.bibtex_key, db.publication.title,  
                                         db.publication.google_scholar_cites, db.publication.impact_factor,
                                         db.authorship_approval.id, db.authorship_approval.name, db.authorship_approval.date_request_sent,
                                         db.authorship_approval.date_approval_given],
                                         orderby = dict(project=db.project.title, dataset=db.dataset.shortname,authorship_approval=db.authorship_approval.id),
                               user_signature=True,maxtextlength =200, csv=False, paginate=150)
      return dict(grid=grid)
#+end_src  
** COMMENT deprecated manageproj-code
#+name:deprecated manageproj
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:deprecated manageproj####

  def manage_projects():
      grid = SQLFORM.smartgrid(db.project,linked_tables=['dataset', 'entity', 'attr','accessrequest', 'keyword'],
                               fields = [db.project.title,
                                         db.dataset.title, db.dataset.creator,
                                         db.entity.entityname,
                                         db.attr.name, db.attr.definition,
                                         db.accessrequest.accessor_id, db.accessrequest.dataset_id,
                                         db.accessrequest.title, 
                                         db.keyword.thesaurus, db.keyword.keyword],
                               user_signature=True,maxtextlength =200)
      return dict(grid=grid)
  
#+end_src

** Form to manage all dataset
*** manage datasets
#+name:manage datasets
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/controllers/forms.py :exports reports :eval no :padline no
  
  def manage_datasets():
      grid = SQLFORM.smartgrid(db.dataset,linked_tables=[ 'entity', 'keyword', 'intellectualright', 'attr','accessrequest', 'publication', 'authorship_approval'
                                                        ],
                               fields = [
                                         db.dataset.shortname,
                                         db.dataset.id,
                              
                                         db.dataset.additional_metadata,                                          
                                         db.dataset.contact_email,
                                         db.entity.entityname, db.entity.entitydescription, db.entity.physical_distribution,
                                         db.attr.variable_name, db.attr.variable_definition,
                                         db.accessrequest.accessdataset_id, 
                                         db.accessrequest.dataset_id,
                                         db.accessrequest.title, 
                                         db.keyword.keyword,
                                         db.intellectualright.licence_code,
                                         db.publication.bibtex_key, db.publication.title,  
                                         db.publication.google_scholar_cites, db.publication.impact_factor,
                                         db.authorship_approval.id, db.authorship_approval.name, db.authorship_approval.date_request_sent,
                                         db.authorship_approval.date_approval_given],
                                         orderby = dict(dataset=db.dataset.title,authorship_approval=db.authorship_approval.id),
                               user_signature=True,maxtextlength =200, csv=False, paginate=50)
      return dict(grid=grid)
#+end_src


** test autoenter creator in contact FAIL

*** COMMENT auto fill contact
#+begin_src R :session *R* :tangle no :exports reports :eval no :padline no

def my_form_processing(grid):
    if grid.vars.contact == "":
       grid.vars.contact = 'a test'

#+end_src
*** manage datasets test
#+name:manage datasets
#+begin_src R :session *R* :tangle no :exports reports :eval no :padline no
  
  def manage_datasets():
      grid = SQLFORM.smartgrid(db.dataset,linked_tables=[ 'entity', 'keyword', 'intellectualright', 'attr','accessrequest', 'publication', 'authorship_approval'
                                                        ],
                               fields = [
                                         db.dataset.shortname,
                                         db.dataset.id,
                              
                                         db.dataset.additional_metadata,                                          
                                         db.dataset.contact_email,
                                         db.entity.entityname, db.entity.entitydescription, db.entity.physical_distribution,
                                         db.attr.variable_name, db.attr.variable_definition,
                                         db.accessrequest.accessdataset_id, 
                                         db.accessrequest.dataset_id,
                                         db.accessrequest.title, 
                                         db.keyword.keyword,
                                         db.intellectualright.licence_code,
                                         db.publication.bibtex_key, db.publication.title,  
                                         db.publication.google_scholar_cites, db.publication.impact_factor,
                                         db.authorship_approval.id, db.authorship_approval.name, db.authorship_approval.date_request_sent,
                                         db.authorship_approval.date_approval_given],
                                         orderby = dict(dataset=db.dataset.title,authorship_approval=db.authorship_approval.id),
                               user_signature=True,maxtextlength =200, csv=False, paginate=50)
      if grid.process(onvalidation=my_form_processing).accepted:
          session.flash = 'record inserted'
          redirect(URL())

      return dict(grid=grid)
#+end_src

** Form to manage all publications
#+name:manage datasets
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/controllers/forms.py :exports reports :eval no :padline no
  
  def manage_publications():
      grid = SQLFORM.smartgrid(db.publication,linked_tables=['publication', 'authorship_approval'
                                                        ],
                               fields = [
                                         db.publication.bibtex_key, db.publication.citation,  
                                         db.publication.publication_status, 
                                         db.authorship_approval.id, db.authorship_approval.name,
                                         db.authorship_approval.date_request_sent,
                                         db.authorship_approval.date_approval_given],
                                         orderby = dict(publication_status=db.publication.publication_status,
                                         authorship_approval=db.authorship_approval.id),
                               user_signature=True,maxtextlength =200, paginate= 150)
      return dict(grid=grid)
#+end_src


** Form to manage accessors or groups
*** COMMENT manage access groups
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/controllers/forms.py :exports reports :eval no :padline no
  
  def manage_accessors_or_groups():
      grid = SQLFORM.smartgrid(db.accessdataset,linked_tables=['accessor'],
                               fields = [
                                         db.accessdataset.name,
                                         db.accessdataset.email,
                                         db.accessor.name, db.accessor.email],
                                         orderby = dict(accessdataset=[db.accessdataset.name]),
                               user_signature=True,maxtextlength =200, csv=False, paginate=35)

      return dict(grid=grid)
      # db.accessor.email.requires = [IS_IN_DB(db,db.accessor.id,'%(email)s')]  
#+end_src

*** COMMENT summarise-access-code
#+name:summarise-access
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:summarise-access####
select foo.*, t3.*
from (
SELECT t1.*, t2.title
  FROM accessrequest t1
  join dataset t2
  on t1.dataset_id = t2.id
  ) foo
  join accessor t3
  on foo.accessor_id = t3.id

#+end_src

** finesse the interface
- the following makes a home page
- TODO the indenting is stuffed in the index chunk

- also go to static/css/web2py.css and change line 33/34 text width to about 800px
- removed from just above instructions and replaced with if auth is logged after instructions.
  {{if 'message' in globals():}}
  <h3>{{=message}}</h3>

- removed from bottom
  {{block right_sidebar}}
  {{=A(T("Administrative Interface"), _href=URL('admin','default','index'), _class='btn',
       _style='margin-top: 1em;')}}
  {{end}}
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/views/default/index.html :exports reports :eval no :padline
  {{left_sidebar_enabled,right_sidebar_enabled=False,('message' in globals())}}
{{extend 'layout.html'}}
  

  <h4>{{=T('Instructions')}}</h4>
  <li>{{=T('To use the data inventory please register in top right corner.')}}</li>
  {{if auth.is_logged_in():}}
<ol>
    <li>{{=T('You are using the data inventory')}}</li>
  <li>{{=XML(T('The main tool for managing research projects (umbrella grouping of datasets) is at %s',
             A('%(application)s/forms/manage_projects/'%request,
           _href=URL('forms','manage_projects'))))}}</li>
    <li>{{=XML(T('The main tool for managing datasets is at %s',
           A('%(application)s/forms/manage_datasets/'%request,
             _href=URL('forms','manage_datasets'))))}}</li>
  <li>{{=XML(T('The technical documentation for this application is at %s',
             A('this link',
           _href=URL('static','index.html'), _target='new')))}}</li>
</ol>
  {{elif 'content' in globals():}}
{{=content}}
  {{else:}}
{{=BEAUTIFY(response._vars)}}
  {{pass}}

  {{block right_sidebar}}
{{=A(T("About this site"), _href=URL('static', 'index.html'), _class='btn',
       _style='margin-top: 1em;')}}
{{end}}
  
#+end_src

* COMMENT load using sql.r-code
#+name:load using sql.r
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:load using sql.r####

#+end_src

* COMMENT migrate DDI R Code
** COMMENT load data-code
#+name:load data
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:load data####
  library(swishdbtools)
  library(sqldf)
  ch <- connect2postgres2("data_inventory_hanigan_dev4")
  
  #pgListTables(ch, "public")
  
  indir <- "~/Dropbox/projects/0.3 Catalogue/backups/csvs/2014-04-15"
  setwd(indir)
  
  # load
  stdy <- read.csv("STDYDSCR_edit.csv", stringsAsFactor = F)
  fdsc <- read.csv("FILEDSCR.csv", stringsAsFactor = F)
  
  # check
  #str(stdy)
  #str(fdsc)
  ids <- names(table(stdy$IDNO))
  ids[grep("GRID", ids)]
  # do
  stdy_i <- "bom_grids"
  stdy_i
  stdyj <- stdy[tolower(stdy$IDNO) == stdy_i, ]
  names(stdyj) <- tolower(names(stdyj))
  #write.csv(t(stdyj), "foo.csv"
  t(stdyj)
  
  cwlk <- dbGetQuery(ch,
  "
  select datinv, DDI_NODE from crosswalk
  where eml_table = 'project' and
  datinv is not null and datinv !=''
  "                   )
  cwlk
  ddi1 <- tolower(cwlk[cwlk$datinv != 'NA',2])
  ddi <- ddi1[-c(which(ddi1 == ''))]
  ddi
  #ddi <- which(names(stdyj) %in% ddi)
  ddi <- paste(ddi , sep = "", collapse = ", ")
  cat(ddi)
  
  datinv <- tolower(cwlk[cwlk$datinv != 'NA',1])
  datinv <- datinv[-c(which(ddi1 == ''))]
  datinv<-paste(datinv, sep = "", collapse = ", ")
  cat(datinv)
  
  dbWriteTable(ch,  "temp", stdyj, row.names = F)
  
  sqltxt <- paste("
  insert into project ( ", datinv ,")
  select  ", ddi ,"
  from temp
  ", sep = "")
  cat(sqltxt)
  
  dbSendQuery(ch, sqltxt)
  dbRemoveTable(ch, "temp")
  
  # manually add dataset, get the dataaset_id
  
  sql <- paste("select t1.IDNO, t2.*
  from stdy t1
  left join fdsc t2
  on t1.IDNO = t2.IDNO
  where lower(t1.IDNO) like '",stdy_i,"'", sep = "")
  #cat(sql)
  dat <- sqldf(sql, drv = "SQLite")
  str(dat)
  dat[,1:4]
  #dat[,'FILENAME']
  names(dat) <- tolower(names(dat))
  #dat
  # give up inserting and just look
  #write.csv(dat, "~/Desktop/temp.csv", row.names = F)
  
#+end_src
** COMMENT snip-code
#+name:snip
#+begin_src R :session *shell* :tangle no :exports none :eval no
#### name:snip ####



  ## for(i in 1:nrow(dat)){
  ## # i = 1
  ## print(as.character(dat[i,'FILENAME']))
  ## qc <-   cbind(names(dat),t(dat[i,]))
  ## #str(qc)
  ## print(qc)
  ## #print(xtable(qc), include.rownames = F, type = 'html')
  ## }
  
  
  
  
  
  
  ## dir(indir)
  ## dat <- read.csv(file.path(indir, "STDYDSCR_edit.csv"), stringsAsFactor = F)
  ## str(dat)
  ## #dbWriteTable(ch, "stdydscr", dat)
  
  ## names(table(dat$IDNO))
  ## names(table(dat$AUTHENTY))
  ## names(table(dat$DISTRBTR))
  
  
  ## stdy <- dat[grep("drought$", tolower(dat$IDNO)),]
  ## t(stdy[,])
  ## #matrix(names(stdy))
  
  
  
  #dat <- read.csv(file.path(indir, "FILEDSCR.csv"))
  #str(dat)
  #names(table(dat$IDNO))
  #dat[grep("mesic", dat$FILENAME),]
  
  #file <- dat[grep("ECOR", dat$IDNO),]
  #file
  
#+end_src
* COMMENT use-EML-code
#+name:use-EML
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:use-EML ####
  
  #title <- "Thresholds and Tipping Points in a Sarracenia 
  #          Microecosystem at Harvard Forest since 2012"
  
  title <- stdyj$titl
  
  #creator <- c(as("Aaron Ellison", "creator"), as("Nicholas Gotelli", "creator"))
  
  creator <- stdyj$authenty
  
  #abstract <- "The primary "
  
  abstract  <- stdyj$abstract
  
  ## contact <- as(aaron, "contact")
  ## contact@address = HF_address
  ## contact@organizationName = "Harvard Forest"
  ## contact@phone = "000-000-0000"
  
  contact <- stdyj$distrbtr
  
  #### PROJECT ####  
  project_title <- stdyj$titl
  
  personnel_individualname <-  ifelse(stdyj$producer == "", stdyj$distrbtr, stdyj$producer)
  #personnel_organisationname
  
  funding  <- stdyj$fundag
  
  ## The `eml_person` function automatically decides whether to return a `contact`, `creator` list, or `associatedParty`, based on the additional information provided (such as an email address in angle braces, contributor role, `ctb` in square brackets).
  ## other_researchers <- eml_person("Benjamin Baiser [ctb]", 
  ##                                 "Jennifer Sirota [ctb]")
  
  associated_party <- ""
  
  keyword <- stdyj$datakind
  
  ## keys <- eml_keyword(list(
  ##  "LTER controlled vocabulary" = c("bacteria", 
  ##                                   "carnivorous plants", 
  ##                                   "genetics", 
  ##                                   "thresholds"),
  ##              "LTER core area" = c("populations", 
  ##                                   "inorganic nutrients", 
  ##                                   "disturbance"),
  ##                 "HFR default" = c("Harvard Forest", 
  ##                                   "HFR", 
  ##                                   "LTER", 
  ##                                   "USA")))
  
  geographicdescription <- stdyj$geogcover
  
  boundingcoordinates <- stdyj$geogunit
  
  temporalcoverage_daterange <- stdyj$proddatestdy
  temporalcoverage_date <- ifelse(stdyj$timeprd == "", stdyj$colldate, stdyj$timeprd)
  
  # dates <- temporalcoverage_date
    
  ## coverage <- eml_coverage(
  ##   scientific_names = "Sarracenia purpurea", 
  ##   dates            = c('2012-06-01', '2013-12-31'),
  ##   geographic_description = "Harvard Forest Greenhouse, 
  ##                             Tom Swamp Tract (Harvard Forest)", 
  ##   NSEWbox          = c( 42.55,  42.42, -72.1, -72.29, 160, 330))
  
  #### Methods
  methods <- ""
  
  library(RWordXML)
  library(XML)
  # f2 <- wordDoc(system.file("examples", "methods.docx",
  # package="EML"))
  f2 <- wordDoc("~/tools/EML/inst/examples/methods.docx")
  doc <- f2[[getDocument(f2)]]
  txt <- xpathSApply(doc, "//w:t", xmlValue)
  ## ## FIXME add <title> <section> and <para> blocking back: 
  method <- paste(txt, collapse = "\n\n")
  cat(method)
  
  methods <- new("methods", methodStep = c(new("methodStep", description = method)))
  print(methods)
  methods_citation             <-   stdyj$biblcitdoc      
  methods_citation_attribution <-   stdyj$biblcitstdy     
  qualitycontrol               <-   stdyj$cleanops        
  sampling_desc                <-   stdyj$anlyunit        
  studyextent                  <-   stdyj$universe
  
  
  additionalMetadata <- stdyj$confdec
    
  #hf205 <- eml_read(system.file("examples", "hf205.xml", package="EML"))
  #additionalMetadata <- hf205@additionalMetadata # extracted from previous eml file
  
  intellectualrights  <- stdyj$copyright
  special_permissions <- stdyj$specperm
  restrictions        <- stdyj$restrctn
  rights <- paste(intellectualrights, special_permissions, restrictions)
  
  #rights <- "This dataset is released to the public "
  
  pubDate <- "2012"
  
  
  dataTable <- eml_dataTable(dat,
                             col.defs = col.defs,
                             unit.defs = unit.defs,
                             description = "Metadata documentation for S1.csv", 
                             filename = "S1.csv")
  
  
  
  ## dataset <- new("dataset", 
  ##                 title = title,
  ##                 creator = creator,
  ##                 contact = contact,
  ##                 pubDate = pubDate,
  ##                 intellectualRights = rights,
  ##                 abstract = abstract,
  ##                 associatedParty = other_researchers,
  ##                 keywordSet = keys,
  ##                 coverage = coverage,
  ##                 methods = methods,
  ##                 dataTable = c(dataTable))
  
  ## eml     <- eml( dataset = dat,
  ##                 title = title,
  ##                 creator = creator,
  ##                 contact = contact,
  ##                 pubDate = pubDate,
  ##                 associatedParty = other_researchers,
  ##                 intellectualRights = rights,
  ##                 abstract = abstract,
  ##                 keywordSet = keys,
  ##                 coverage = coverage,
  ##                 methods = method,
  ##                 additionalMetadata = additionalMetadata
  ##               )
  title 
  creator 
  contact 
  pubDate 
  #associatedParty
  associated_party
  #  other_researchers
  #intellectualRights
   rights
  abstract
  #keywordSet =
    keys
  coverage
  methods 
  additionalMetadata
  
  
  
  eml_write(eml, file="hf205_from_EML.xml")
  eml_validate("hf205_from_EML.xml")
#+end_src

* COMMENT use-EML-func
#+name:use-EML
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:use-EML ####
  add_eml_project <- function(indir){
  
  #### PROJECT ####  
  project_title <- readline("umbrella project title: ")
  
  personnel_data_owner <- readline("personnel data owner (individual, role or organisation): ")
  personnel_owner_organisationname <- readline("personnel owner organisation name: ")
  
  ## The `eml_person` function automatically decides whether to return a `contact`, `creator` list, or `associatedParty`, based on the additional information provided (such as an email address in angle braces, contributor role, `ctb` in square brackets).
  ## other_researchers <- eml_person("Benjamin Baiser [ctb]", 
  ##                                 "Jennifer Sirota [ctb]")

  funding  <- readline("funding: ")
  project_abstract <- readline("project abstract: ")
  studyareadescription <- readline("study area description: ")
  project_established <- readline("project established: ")
  project_citation <- readline("project citation: ")
  
  keyword <- readline("keywords: ")
  
  ## keys <- eml_keyword(list(
  ##  "LTER controlled vocabulary" = c("bacteria", 
  ##                                   "carnivorous plants", 
  ##                                   "genetics", 
  ##                                   "thresholds"),
  ##              "LTER core area" = c("populations", 
  ##                                   "inorganic nutrients", 
  ##                                   "disturbance"),
  ##                 "HFR default" = c("Harvard Forest", 
  ##                                   "HFR", 
  ##                                   "LTER", 
  ##                                   "USA")))
  return(eml_project)
  }
  
  add_eml_dataset <- function(indir = getwd(), eml_validate = FALSE){
  
  shortname <- readline("shortname: ")
    
  #title <- "Thresholds and Tipping Points in a Sarracenia 
  #          Microecosystem at Harvard Forest since 2012"
  
  title <- readline("title: ")
  
  #creator <- c(as("Aaron Ellison", "creator"), as("Nicholas Gotelli", "creator"))
  
  creator <- readline("creator: ")
  
 
  ## contact <- as(aaron, "contact")
  ## contact@address = HF_address
  ## contact@organizationName = "Harvard Forest"
  ## contact@phone = "000-000-0000"
  
  contact <- readline("contact: ")
  contact_email <- readline("contact email: ")

  #abstract <- "The primary "
  
  abstract  <- readline("abstract: ")
  
  associated_party <- readline("associated_party: ")
  
  geographicdescription <- readline("geographic description: ")
  
  boundingcoordinates <- readline("bounding coordinates in order N, S, E, W (Optionally also add altitudeMinimum, altitudeMax): ")
  
  temporalcoverage_daterange <- readline("temporalcoverage date range: ")
  temporalcoverage_begindate <- readline("temporalcoverage begin date: ")
  temporalcoverage_enddate <- readline("temporalcoverage end date: ")
  # dates <- temporalcoverage_date
    
  taxonomic_coverage <- readline("taxonomic coverage: ")
  ## coverage <- eml_coverage(
  ##   scientific_names = "Sarracenia purpurea", 
  ##   dates            = c('2012-06-01', '2013-12-31'),
  ##   geographic_description = "Harvard Forest Greenhouse, 
  ##                             Tom Swamp Tract (Harvard Forest)", 
  ##   NSEWbox          = c( 42.55,  42.42, -72.1, -72.29, 160, 330))
  
  #### Methods

  
  ## library(RWordXML)
  ## library(XML)
  ## f2 <- wordDoc(system.file("examples", "methods.docx", package="EML"))
  ## doc <- f2[[getDocument(f2)]]
  ## txt <- xpathSApply(doc, "//w:t", xmlValue)
  ## ## FIXME add <title> <section> and <para> blocking back: 
  ## method <- paste(txt, collapse = "\n\n")
  
  studyextent                  <-   readline("study extent: ")
  sampling_desc                <-   readline("sampling desc: ") 
  
  #methods <- new("methods", methodStep = c(new("methodStep", description = method)))
  method_steps <- readline("methods_and_sampling_information: ")  
  methods_citation             <-   readline("method citation: ")
  #methods_citation_attribution <-   stdyj$biblcitstdy     
  #qualitycontrol               <-   stdyj$cleanops        
  
  
  additional_metadata <- readline("additional metadata: ")
  additionalinfo <- readline("additionalinfo: ")
  alternateidentifier <- readline("alternate identifier: ")
  #hf205 <- eml_read(system.file("examples", "hf205.xml", package="EML"))
  #additionalMetadata <- hf205@additionalMetadata # extracted from previous eml file

  pubDate <- "2012"

  metadataprovider <- readline("metadataprovider: ")

  # intellectualrights
  licence_code <- readline("licence code: ") 
  licence_text <- readline("licence_text: ")
  special_conditions <- readline("special conditions: ")
  #restrictions        <- stdyj$restrctn
  #rights <- paste(intellectualrights, special_conditions, restrictions)
  
  #rights <- "This dataset is released to the public "
  

  
  
  dataTable <- eml_dataTable(dat,
                             col.defs = col.defs,
                             unit.defs = unit.defs,
                             description = "Metadata documentation for S1.csv", 
                             filename = "S1.csv")
  
  
  
  ## dataset <- new("dataset", 
  ##                 title = title,
  ##                 creator = creator,
  ##                 contact = contact,
  ##                 pubDate = pubDate,
  ##                 intellectualRights = rights,
  ##                 abstract = abstract,
  ##                 associatedParty = other_researchers,
  ##                 keywordSet = keys,
  ##                 coverage = coverage,
  ##                 methods = methods,
  ##                 dataTable = c(dataTable))
  
  ## eml     <- eml( dataset = dat,
  ##                 title = title,
  ##                 creator = creator,
  ##                 contact = contact,
  ##                 pubDate = pubDate,
  ##                 associatedParty = other_researchers,
  ##                 intellectualRights = rights,
  ##                 abstract = abstract,
  ##                 keywordSet = keys,
  ##                 coverage = coverage,
  ##                 methods = method,
  ##                 additionalMetadata = additionalMetadata
  ##               )
  title 
  creator 
  contact 
  pubDate 
  #associatedParty
  associated_party
  #  other_researchers
  #intellectualRights
   rights
  abstract
  #keywordSet =
    keys
  coverage
  methods 
  additionalMetadata
  
  
  
  #eml_write(eml, file="hf205_from_EML.xml")
  #eml_validate("hf205_from_EML.xml")
  
  }
#+end_src


* COMMENT DEPRECATED old crap
** COMMENT load old access data
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:asdf####
  library(stringr)
  library(swishdbtools)
  library(sqldf)
  ch <- connect2postgres2("data_inventory2")
  pgListTables(ch, "public")
  
  dat2 <- dbGetQuery(ch, "select * from project")
  str(dat2)
  head(dat2)
  names(dat2)
  
  
  fpath1 <- dir("/home/ivan_hanigan/Dropbox/data/ltern_data_inventory-backups", pattern = "csv", full.names=T)
  fpath1
  flist <-  as.data.frame(matrix(    unlist(str_split(fpath1,"-"))                       , ncol =9, byrow=T))
  flist[,3:6]
  head(flist)
  flist$date <- as.Date(
    paste(flist$V4,"-", flist$V5,"-", flist$V6,sep = "")
    )
  infile <- flist[which(flist$date == max(flist$date)),]
  infiles <- fpath1[which(flist$date == max(flist$date))]
  infiles
  
  
  # for
  i <- 3
  fpath <-infiles[i]
  fpath
  
  dat <- read.csv(fpath, stringsAsFactors = F)
  str(dat)
  dat
  
  names(dat)
  
  datx <- as.data.frame(table(dat$pn_code_broad_group))
  datx
  for(i in 2:nrow(datx)){
  #  i <- 2
  t(datx[i,1]  )
  sql <- sprintf(
    "INSERT INTO project(
              title)
      VALUES ('%s')
    ", datx$Var1[i])
  cat(sql)
  dbSendQuery(ch, sql)
  }
  
  
  infiles
  i <- 2
  fpath <-infiles[i]
  fpath
  
  dat <- read.csv(fpath, stringsAsFactors = F)
  str(dat)
  #dat
  
  names(dat)
  
  
  for(i in 2:nrow(dat)){
    i <- 1
  pid <- dbGetQuery(ch,
                    sprintf(
                      "select id from project where title = %s",
                      )
  
    
  sql <- sprintf(
     "INSERT INTO dataset(
              id, project_id, title, creator, contact, metadataprovider, intellectualright, 
              pubdate, temporalcoverage, boundingcoordinates, geographicdescription, 
              abstract)
      VALUES (?, ?, ?, ?, ?, ?, ?, 
              ?, ?, ?, ?, 
              ?)"
  , dat$refid[i], dat$data_package_title[i], dat$notes[i], dat$contact_name[i])
  cat(sql)
  dbSendQuery(ch, sql)
  }
  
  
  for(i in 2:nrow(dat)){
  #  i <- 1
  sql <- sprintf(
  "INSERT INTO dataset(
              id, project_id, title, creator, contact, metadataprovider, intellectualright, 
              pubdate, temporalcoverage, boundingcoordinates, geographicdescription, 
              abstract)
      VALUES (?, ?, ?, ?, ?, ?, ?, 
              ?, ?, ?, ?, 
              ?);
    ", dat$refid[i], dat$data_package_title[i], dat$notes[i], dat$contact_name[i])
  cat(sql)
  dbSendQuery(ch, sql)
  }
  
#+end_src
** COMMENT first attempt at data inventory tables

*** COMMENT models/db.py-code
- 
- The first bit to change is the db reference from SQLite to postgres
#+begin_src markdown :tangle no :exports reports :eval no :padline no
  # -*- coding: utf-8 -*-
  
  #########################################################################
  ## This scaffolding model makes your app work on Google App Engine too
  ## File is released under public domain and you can use without limitations
  #########################################################################
  
  ## if SSL/HTTPS is properly configured and you want all HTTP requests to
  ## be redirected to HTTPS, uncomment the line below:
  # request.requires_https()
  
  if not request.env.web2py_runtime_gae:
      ## if NOT running on Google App Engine use SQLite or other DB
      # db = DAL('sqlite://storage.sqlite',pool_size=1,check_reserved=['all'])
      db = DAL("postgres://w2p_user:xpassword@localhost:5432/data_inventory")
  else:
      ## connect to Google BigTable (optional 'google:datastore://namespace')
      db = DAL('google:datastore')
      ## store sessions and tickets there
      session.connect(request, response, db=db)
      ## or store session in Memcache, Redis, etc.
      ## from gluon.contrib.memdb import MEMDB
      ## from google.appengine.api.memcache import Client
      ## session.connect(request, response, db = MEMDB(Client()))
  
  ## by default give a view/generic.extension to all actions from localhost
  ## none otherwise. a pattern can be 'controller/function.extension'
  response.generic_patterns = ['*'] #if request.is_local else []
  ## (optional) optimize handling of static files
  # response.optimize_css = 'concat,minify,inline'
  # response.optimize_js = 'concat,minify,inline'
  ## (optional) static assets folder versioning
  # response.static_version = '0.0.0'
  #########################################################################
  ## Here is sample code if you need for
  ## - email capabilities
  ## - authentication (registration, login, logout, ... )
  ## - authorization (role based authorization)
  ## - services (xml, csv, json, xmlrpc, jsonrpc, amf, rss)
  ## - old style crud actions
  ## (more options discussed in gluon/tools.py)
  #########################################################################
  
  from gluon.tools import Auth, Crud, Service, PluginManager, prettydate
  auth = Auth(db)
  crud, service, plugins = Crud(db), Service(), PluginManager()
  
  ## create all tables needed by auth if not custom tables
  auth.define_tables(username=False, signature=False)
  
  ## configure email
  mail = auth.settings.mailer
  mail.settings.server = 'logging' or 'smtp.gmail.com:587'
  mail.settings.sender = 'you@gmail.com'
  mail.settings.login = 'username:password'
  
  ## configure auth policy
  auth.settings.registration_requires_verification = False
  auth.settings.registration_requires_approval = False
  auth.settings.reset_password_requires_verification = True
  
  ## if you need to use OpenID, Facebook, MySpace, Twitter, Linkedin, etc.
  ## register with janrain.com, write your domain:api_key in private/janrain.key
  from gluon.contrib.login_methods.rpx_account import use_janrain
  use_janrain(auth, filename='private/janrain.key')
  
  #########################################################################
  ## Define your tables below (or better in another model file) for example
  ##
  ## >>> db.define_table('mytable',Field('myfield','string'))
  ##
  ## Fields can be 'string','text','password','integer','double','boolean'
  ##       'date','time','datetime','blob','upload', 'reference TABLENAME'
  ## There is an implicit 'id integer autoincrement' field
  ## Consult manual for more options, validators, etc.
  ##
  ## More API examples for controllers:
  ##
  ## >>> db.mytable.insert(myfield='value')
  ## >>> rows=db(db.mytable.myfield=='value').select(db.mytable.ALL)
  ## >>> for row in rows: print row.id, row.myfield
  #########################################################################
  
  ## after defining tables, uncomment below to enable auditing
  # auth.enable_record_versioning(db)
  
  # db.define_table('dataset',
  #   Field('pn_code', 'string'),
  #   Field('plot_network_study_name', 'string'),
  #   Field('dataset', 'string'),
  #   Field('tern_type', 'string'),
  #   Field('ltern_publ_url','string'),
  #   Field('abstract', 'text')
  # )
  
  db.define_table('data_inventory',
      Field('id2', 'integer'),
      Field('plot_network_study_name','text'),
      Field('pn_group','text'),
      Field('pn_code','text'),
      Field('data_custodian','text'),
      Field('plot_network','text'),
      Field('pi','text'),
      Field('data_custodian_pl_pi','text'),
      Field('data_custodian_organisation','text'),
      Field('data_type','text'),
      Field('notes_issues','text'),
      Field('start_date','integer'),
      Field('end_date','integer'),
      Field('current_status','text'),
      Field('sites_plots','integer'),
      Field('collection_timeframes','text'),
      Field('ecosystem_mvg_numbers','integer'),
      Field('mvg_names','text'),
      Field('tern_type','text'),
      Field('data_interview_status','text'),
      Field('data_interview_date','date'),
      Field('intellectualright_status','text'),
      Field('intellectualright_status_date','date'),
      Field('licence_code','text'),
      Field('access_restrictions','text'),
      Field('estimate_timeframe_data_ready_by_plot','date'),
      Field('date_data_expected_by_ltern','date'),
      Field('date_data_received_by_ltern','date'),
      Field('stored_at','text'),
      Field('eda_status','text'),
      Field('eda_status_date','date'),
      Field('metadata_status','text'),
      Field('metadata_status_date','date'),
      Field('publishing','text'),
      Field('date_published','date'),
      Field('estimated_effort','text'),
      Field('allocated_to','text'),
      Field('depends_on','text'),
      Field('todo_or_done','text'))
  
  #### projects and datasets
  db.define_table(
      'project',
      Field('title', 'string'),
      Field('abstract', 'text')
      )
  
  #### ONE (project) TO MANY (datasets)
  
  db.define_table(
      'dataset',
      Field('project_id',db.project),
      Field('title','string'),
      Field('creator', 'string')
      )
  
  #### ONE (dataset) TO MANY (attr/variables)
  
  db.define_table(
      'attr',
      Field('dataset_id',db.dataset),
      Field('name','string'),
      Field('definition', 'string')
      )
  
#+end_src
*** COMMENT controllers/default.py-code
#+name:controllers/default.py
#+begin_src R :session *R* :tangle no :exports none :eval no :padline no
  # -*- coding: utf-8 -*-
  # this file is released under public domain and you can use without limitations
  
  #########################################################################
  ## This is a sample controller
  ## - index is the default action of any application
  ## - user is required for authentication and authorization
  ## - download is for downloading files uploaded in the db (does streaming)
  ## - call exposes all registered services (none by default)
  #########################################################################
  
  
  def index():
      """
      example action using the internationalization operator T and flash
      rendered by views/default/index.html or views/generic.html
  
      if you need a simple wiki simply replace the two lines below with:
      return auth.wiki()
      """
      response.flash = T("Welcome to web2py!")
      return dict(message=T('Hello World'))
   
  
  def user():
      """
      exposes:
      http://..../[app]/default/user/login
      http://..../[app]/default/user/logout
      http://..../[app]/default/user/register
      http://..../[app]/default/user/profile
      http://..../[app]/default/user/retrieve_password
      http://..../[app]/default/user/change_password
      http://..../[app]/default/user/manage_users (requires membership in
      use @auth.requires_login()
          @auth.requires_membership('group name')
          @auth.requires_permission('read','table name',record_id)
      to decorate functions that need access control
      """
      return dict(form=auth())
  
  @cache.action()
  def download():
      """
      allows downloading of uploaded files
      http://..../[app]/default/download/[filename]
      """
      return response.download(request, db)
  
  
  def call():
      """
      exposes services. for example:
      http://..../[app]/default/call/jsonrpc
      decorate with @services.jsonrpc the functions to expose
      supports xml, json, xmlrpc, jsonrpc, amfrpc, rss, csv
      """
      return service()
  
  
  @auth.requires_signature()
  def data():
      """
      http://..../[app]/default/data/tables
      http://..../[app]/default/data/create/[table]
      http://..../[app]/default/data/read/[table]/[id]
      http://..../[app]/default/data/update/[table]/[id]
      http://..../[app]/default/data/delete/[table]/[id]
      http://..../[app]/default/data/select/[table]
      http://..../[app]/default/data/search/[table]
      but URLs must be signed, i.e. linked with
        A('table',_href=URL('data/tables',user_signature=True))
      or with the signed load operator
        LOAD('default','data.load',args='tables',ajax=True,user_signature=True)
      """
      return dict(form=crud())
  
  def entry_datasets():
      """returns a form where the can entry a post"""
      form = crud.create(db.data_inventory)
      return dict(form=form)
  
  #def search_dogs():
  #    form, records = crud.search(db.datainventory)
  #    return dict(form=form, records=records)
  
  def search_dogs():
      return dict(form=SQLFORM.grid(db.data_inventory, user_signature=True, maxtextlength =200,
                                    fields = [db.data_inventory.id, db.data_inventory.plot_network_study_name, db.data_inventory.pn_group, db.data_inventory.data_type, db.data_inventory.eda_status_date]))
  
  
  def search_datasets():
      return dict(form=SQLFORM.grid(db.dataset.id==db.data_inventory.id2, user_signature=True, maxtextlength =200,
                                    fields = [db.dataset.id, db.dataset.plot_network_study_name, db.dataset.pn_code, db.dataset.dataset, db.dataset.tern_type, db.data_inventory.notes_issues]))
  
  
  
  def manage_projects():
      grid = SQLFORM.smartgrid(db.project,linked_tables=['dataset', 'attr'],
                               fields = [db.project.title,
                                         db.dataset.title, db.dataset.creator,
                                         db.attr.name, db.attr.definition],
                               user_signature=True)
      return dict(grid=grid)
  
#+end_src
** COMMENT floating tooltips-code
#+name:floating tooltips
#+begin_src R :session *R* :tangle no :exports none :eval no

#### name:floating tooltips####
# if we want floating tooltips
# http://www.web2pyslices.com/slice/show/1418/auto-tooltip-in-forms
You will need a jquery plugin to show the tip so you may google for it and pick one. Or you can use this http://jquery.bassistance.de/tooltip/jquery.tooltip.zip See it what it looks like here http://jquery.bassistance.de/tooltip/demo/

Extract jquery.tooltip.min.js and jquery.tooltip.css to static folder. Edit web2py_ajax.html and include the call for these files. Something like this:

....
# to views/web2py_ajax.html
<script type="text/javascript"><!--
    // These variables are used by the web2py_ajax_init function in web2py_ajax.js (which is loaded below).
    var w2p_ajax_confirm_message = "{{=T('Are you sure you want to delete this object?')}}";
    var w2p_ajax_disable_with_message = "{{=T('Working...')}}";
    var w2p_ajax_date_format = "{{=T('%Y-%m-%d')}}";
    var w2p_ajax_datetime_format = "{{=T('%Y-%m-%d %H:%M:%S')}}";
    var ajax_error_500 = '{{=T.M('An error occured, please [[reload %s]] the page') % URL(args=request.args, vars=request.get_vars) }}'
    //--></script>
{{
response.files.insert(0,URL('static','js/jquery.js'))
response.files.insert(1,URL('static','css/calendar.css'))
response.files.insert(2,URL('static','js/calendar.js'))
response.files.insert(3,URL('static','js/web2py.js'))
response.files.insert(3,URL(r=request,c='static',f='jquery.tooltip.min.js'))
response.files.insert(3,URL(r=request,c='static',f='jquery.tooltip.css'))
response.include_meta()
response.include_files()
}}

<script type="text/javascript">
       $(function() {
               $(".w2p_fw").each(function (){                    //iterates over all form widgets
                       $(this).attr('title',$(this).next().html());      // set title for the widget taken from the comment column
                       $(this).next().html('');                             // clear the comment
                       $(this).tooltip();                                    // create the tooltip with title attribute set
               });
       });
</script>

#+end_src
* COMMENT crosswalk-code
#+name:crosswalk
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:crosswalk####
  library(gdata)
  library(disentangle)
  library(rpostgrestools)
  ch <- connect2postgres2("data_inventory_hanigan_dev3")
  ch2 <- connect2postgres2("data_inventory_hanigan_dev4")
  dat <- dbGetQuery(ch, "select * from crosswalk")
  str(dat)
  dbWriteTable(ch2, "crosswalk2", dat, append = F, row.names=F)
  
  # OR
  '
  sourcedb <- connect2postgres2("data_inventory_hanigan_dev4")
  targetdb <- connect2postgres2("data_inventory_air_pollution")
  dat <- dbGetQuery(sourcedb, "select * from crosswalk")
  #str(dat)
  dbSendQuery(targetdb, "delete from crosswalk2")
  dbWriteTable(targetdb, "crosswalk2", dat, append = T, row.names=F)
  dbSendQuery(targetdb, "delete from crosswalk")
  #paste(names(dat), sep = "", collapse = ", ")
  dbSendQuery(targetdb, "insert into crosswalk (eml_module, eml_table, datinv, eml_node, help_comment, eml_desc, eml_standard_link, eml_local_link, ddi_module, ddi_node, morpho, ltern_table, ltern_name, portal_ddf_qaf, ltern_desc, aekos_shared, aekos_desc, asn, tern, ala, psql_type, w2p_code, constraint_text, lter_manual_page, transfer2new)
  select eml_module, eml_table, datinv, eml_node, help_comment, eml_desc, eml_standard_link, eml_local_link, ddi_module, ddi_node, morpho, ltern_table, ltern_name, portal_ddf_qaf, ltern_desc, aekos_shared, aekos_desc, asn, tern, ala, psql_type, w2p_code, constraint_text, lter_manual_page, transfer2new from crosswalk2")
  dbSendQuery(targetdb, "delete from crosswalk2")
  '
  
  # OLD CRAP
  fi <- dir("../emldb", full.names = T)
  fi
  dat  <- read.xls(fi[2])
  str(dat)
  cat(
  paste(
    lcu(names(dat))
    , collapse = "','string'),\n", sep = "")
  )
  # dbWriteTable(ch, "crosswalk", dat, append = T)
  # deprecated, now using the hanigan db
  dat <- dbReadTable(ch, "crosswalk")
  ch2  <- connect2postgres2("data_inventory_hanigan_dev")
  str(dat)
  dbWriteTable(ch2, "crosswalk", dat, append = F, row.names=F)
  
  # backups
  dir("../emldb")
  dat <- dbReadTable(ch, "crosswalk")
  str(dat)
  write.table(dat, file.path("../emldb", "crosswalk-2015-02-22.csv"), sep = ",", row.names = F)
#+end_src  
** COMMENT crosswalk-code
#+name:crosswlak 

#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
   
  db.define_table(
      'crosswalk',
      Field('eml_module','string'),
      Field('eml_table','string'),
      Field('datinv','string'),
      Field('eml_node','string'),
      Field('help_comment','text'),
      Field('eml_desc','text'),
      Field('eml_standard_link','string'),
      Field('eml_local_link','string'),
      Field('ddi_module','string'),
      Field('ddi_node','string'),
      Field('morpho','string'),
      Field('ltern_table','string'),
      Field('ltern_name','string'),
      Field('portal_ddf_qaf','string'),
      Field('ltern_desc','text'),
      Field('aekos_shared','string'),
      Field('aekos_desc','text'),
      Field('asn','string'),
      Field('tern','string'),
      Field('ala','string'),
      Field('psql_type','string'),
      Field('w2p_code','string'),
      Field('constraint_text','string'),
      Field('lter_manual_page','string'),
      Field('transfer2new','string')
      )
#+end_src

** Form to manage crosswalk

#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/controllers/forms.py :exports reports :eval no :padline no
  
  def manage_crosswalk():
      grid = SQLFORM.smartgrid(db.crosswalk,
                               fields = [db.crosswalk.transfer2new,   
                                         db.crosswalk.eml_node,
                                         db.crosswalk.datinv,
                                         db.crosswalk.ltern_name,
                                         db.crosswalk.portal_ddf_qaf,
                                         db.crosswalk.ddi_node,
                                         db.crosswalk.aekos_shared
                                         ],
                                         orderby = dict(crosswalk=[db.crosswalk.transfer2new, db.crosswalk.portal_ddf_qaf, db.crosswalk.eml_node]),
                               user_signature=True,maxtextlength =200)

      return dict(grid=grid)

#+end_src

* COMMENT summarise-access-code
#+name:summarise-access
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:summarise-access####
  library(swishdbtools)
  ch <- connect2postgres2("data_inventory_hanigan_dev4")
  accessid <- dbGetQuery(ch,
  "
  select foo.*, t3.*
  from (
    SELECT t1.*, t2.title
    FROM accessrequest t1
    join dataset t2
    on t1.dataset_id = t2.id
    ) foo
    left join accessor t3
    on foo.accessdataset_id = t3.id
  ")
  library(knitr)
  
  write.csv(accessid[,4:5], '~/ethics.csv', row.names = F)
#+end_src

* COMMENT keyword-code
#+name:keyword
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:keyword####
  library(gdata)
  library(disentangle)
  library(rpostgrestools)
  ch <- connect2postgres2("data_inventory_hanigan_dev4")
  fi <- dir("~/Dropbox/data/ltern_data_inventory-backups", full.names = T, pattern = "packages-2015-04")
  fi
  dat  <- read.csv(fi[9], stringsAsFactor = F)
  #str(dat)
  names(dat)
  ky <- sqldf("select data_package_type
   from dat
    group by data_package_type
    order by data_package_type",
              drv= "SQLite")
  names(ky) <- "keyword"
  head(ky)
  #dbRemoveTable(ch, "thesaurus_ltern")
  dbWriteTable(ch, "thesaurus_ltern",
               ky, append = T, row.names=T)
  
#+end_src  
** table
#+begin_src markdown :tangle ~/tools/web2py/applications/data_inventory/models/db.py :exports reports :eval no :padline no
  
  # thesaurus
  
  db.define_table(
      'thesaurus_ltern',
      Field('keyword', 'string')
      )
    
#+end_src

** Form to manage keyword


#+begin_src R :session *R* :tangle ~/tools/web2py/applications/data_inventory/controllers/forms.py :exports reports :eval no :padline no
  
  def manage_thesaurus_ltern():
      grid = SQLFORM.smartgrid(db.thesaurus_ltern,
                               fields = [   
                                         db.thesaurus_ltern.keyword
                                         ],
                                         orderby = dict(thesaurus_ltern=[ db.thesaurus_ltern.keyword]),
                               user_signature=True,maxtextlength =200)

      return dict(grid=grid)

#+end_src

* COMMENT backups
** COMMENT backup-code
#+name:backup
#+begin_src R :session *R* :tangle no :exports none :eval no
  require(swishdbtools)
  ch <- connect2postgres2("data_inventory_hanigan_dev4")  
  tbls <- pgListTables(ch, "public")
  tbls$no <- 1:nrow(tbls)
  tbls
  #tbls[c(1:4,9,12:21),1]
  #tbls <- tbls[c(1:4,9,12:21),1]
  tbls <- tbls[,1]
  for(tbl_i in tbls){
  #  tbl_i = tbls[1]
  dat <- dbGetQuery(ch, sprintf("select * from public.%s order by id", tbl_i))
  #  str(dat)
  write.csv(dat,
            file.path("~/Dropbox/data/data_inventory_hanigan/backups",
                      paste("backup-",tbl_i,"-",
                            gsub(":", "-", gsub(" ", "-", Sys.time()))
                            , ".csv", sep = ""))
            , row.names = F, na = ''
            )
  }
#+end_src

#+RESULTS: backup


** COMMENT pgdump-local
#+name:pgdump
#+begin_src R :session *R* :tangle no :exports none :eval yes
  cat(sprintf('
  pg_dump -h localhost -p 5432 -U postgres -F t -v -i -f "/home/ivan_hanigan/Dropbox/data/data_inventory_hanigan/backups/backup_file-%s.backup" -n "public" data_inventory_hanigan_dev4
  ',gsub(":","-",gsub(" ","-",Sys.time()))))
    
#+end_src

#+RESULTS: pgdump

* COMMENT ETL
** COMMENT go-code
#+name:go
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:go ####
  library(rmarkdown)
  library(knitr)
  library(disentangle)
  setwd("~/tools/web2py/applications/data_inventory/static/")
  dir()
  prj <- "ABS_data"
  #"ROSS RIVER VIRUS RATES"
    #"Biomass_Smoke_Validated_Events"
    #"Eco-social observatories: Tipping points" #"Air_pollution_modelling_LUR_LCT"  # "Air pollution modelling: LUR_Western_Sydney"  "Air_Pollution_Monitoring_Stations_NSW" #
  #dset <- #"Vally2012-RossRiverRates"
  #dset <- "ASGC_1981_2010"
  dsets  <- c("ABS_SEIFA",
  "ASGM_mortality",
  "ASGC_1981_2010",
  "ABS_meshblocks",
  "ABS Correspondence",
  "ABS_Census_2001",
  "ABS_POA",
  "ABS_Census_2011")
  for(i in 1:length(dsets)){
  #  i = 1
  dset  <- dsets[i]
    #"Vally20091223_RossRiverRates Restricted Data"
    #"biomass_smoke_events_db"
  #"ESTPDB" # "LUR_LCT_passive_samplers_2006_2008" #"LUR_Western_Sydney_passive_samplers"  # "AP_monitor_NSW_2014_2015" #  "biomass_smoke_events_database"
  # walkability_osm  
  entities <- T
  show_exemplar <- T
  #render("data_deposit_form.Rmd") 
  # knitr::knit2html("data_deposit_form.Rmd", stylesheet='custom.css')
  #browseURL("data_deposit_form.html")
  #knitr::knit2html("EML_metadata_fields.Rmd", stylesheet='custom.css')
  #rmarkdown::render("EML_metadata_fields.Rmd") #, stylesheet='custom.css')
   #browseURL("EML_metadata_fields.html")
  rmarkdown::render("EML_metadata_short.Rmd") #, stylesheet='custom.css')
  #browseURL("EML_metadata_short.html")
  
  outfile  <- paste(lcu(dset), "_metadata.html", sep = "")
  outfile <- file.path("~/projects_environment_general_local", prj,
  dset, outfile)
  #outfile <- file.path("~/projects", prj, dset, outfile)
  outfile
  #file.rename("EML_metadata_fields.html", outfile)
  file.rename("EML_metadata_short.html", outfile)
  
  }  
  setwd(dirname(outfile))
  browseURL(basename(outfile))
  setwd("~/tools/web2py/applications/data_inventory/static/")
  # no good, do in word? system("pandoc -i data_deposit_form.html -o data_deposit_form.docx")
#+end_src

#+RESULTS: go
: TRUE

** summary of data for ETL
#+begin_src R :session *R* :tangle data_deposit_form.Rmd :exports none :eval no :padline no
  ---
  title: DDF
  output: html_document
  ---
  
  # Introduction
    
  # Project level information
  
  ```{r, echo = F, eval = T, results="hide"}
  #### name:summary of project info ####
  if(exists('ch'))   dbDisconnect(ch)
  library(swishdbtools)
  library(sqldf)
  library(knitr)
  library(xtable)  
  
  ch  <- connect2postgres2("data_inventory_hanigan_dev4")
  prj <- "IUCNmountainashforests_data"
  dset <- "fire100_year"
  
  ```
  ```{r, echo = F, eval = T, results="asis"}  
  
  dat <- sqldf(connection = ch,
    sprintf("select t1.*
    from project t1
    where t1.title = '%s'", prj)
    )
  #names(dat)
  #t(dat)
  
  ####  help
  help  <- sqldf(connection = ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%project%'"
    )
  # head(help)
  
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
  #dat_i
  dat_i$order <- 1:nrow(dat_i)  
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  names(qc2) <- c("variable", "value", "help_comment")
  
  qc2[,2] <- gsub("\n", " | ", qc2[,2])
  print(xtable(qc2), type = "html", include.rownames = F)
  
  ```
  
  # Dataset level information (data packages)
  ```{r, echo = F, eval = T, results="asis"}
  
  #### for each dataset
  #dat$shortname
  # for(i in 1:nrow(dat)){
  
  # i = which(dat$shortname == dset)
  
  
  dat <- dbGetQuery(ch,
  sprintf("select * from dataset
   where shortname ='%s'", dset)
  )
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
  # dat_i
   dat_i$order <- 1:nrow(dat_i)
    #title <- paste(c(as.character(dat_i[dat_i$V1 %in% c('shortname','title'),2])),
    #      collapse = ", ", sep = "")
    #title
  
  help  <- dbGetQuery(ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%dataset%'"
    )
  # head(help)
  
  
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  #qc[1,]
  #names(qc)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  #qc2
  #qc3 <- data.frame(index1 = rep(paste("0. dataset", dset), nrow(qc2)),
  #           index2 = c(title, rep("", nrow(qc2) - 1)),
  #           metadata = qc2)
  #names(qc3) <- c("index1", "index2", "variable", "value", "help")
  names(qc2) <- c("variable", "value", "help")
  #names(qc2)
  #### Keyword
  ky <- dbGetQuery(ch,
    #cat(q
    paste("select t3.keyword
    from dataset t1
    join keyword t3
    on t1.id = t3.dataset_id
    where t1.shortname = '",dset,"'
    ", sep = "")
  )
  
  if(nrow(ky) > 0){
  ky <- ky[,1]
  } else {
  ky <- ''
  }
  ky <- paste(ky, sep = "", collapse=", ")
  ky <- data.frame(variable = "keywords", value = ky, help="Keywords or phrases that concisely describe the resource. Example is biodiversity. Use a controlled vocabulary thesaurus")
  
  
  qc_out <- rbind(qc2, ky)
  #qc_out[,1:3]
  #qc_out
  
  #kable(qc_out, row.names = F)
  
  
  dat <- dbGetQuery(ch,
  sprintf("select t1.*
  from intellectualright t1
  join dataset t2
  on t1.dataset_id =  t2.id
   where shortname ='%s'", dset)
  )
  if(nrow(dat) == 0){
  dat <- data.frame(id = '', dataset_id = '', licence_code = '',
    licence_text = '', special_conditions='')
  }
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
   #dat_i
   dat_i$order <- 1:nrow(dat_i)
    #title <- paste(c(as.character(dat_i[dat_i$V1 %in% c('shortname','title'),2])),
    #      collapse = ", ", sep = "")
    #title
  
  help  <- sqldf(connection = ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%intellectualright%'"
    )
  # head(help)
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  names(qc2) <- c("variable", "value", "help")
  # names(  qc2)
  qc_out <- rbind(qc_out, qc2[-c(1,2),])
  qc_out[,2] <- gsub("\n", " | ", qc_out[,2])
  print(xtable(qc_out), type = "html", include.rownames = F)
  ```
  
  # Entity level information (files)  
  
  ```{r, echo = F, eval = T, results="asis"}
    
  #### entity ####
  #dat <- dbGetQuery(ch, "select * from entity")
  dat_ent <- dbGetQuery(ch,
  #cat(
  sprintf("select 
  t3.*
  from project t1
  join dataset t2
  on t1.id = t2.project_id
  join entity t3
  on t2.id = t3.dataset_id
  where t1.title = '%s'
  and t2.shortname = '%s'", prj, dset),
  )
  # head(dat_ent)
  
  help_ent  <- sqldf("select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%entity%'",
    connection = ch)
  #help_ent
  
  
  for(j in 1:nrow(dat_ent)){
  #j = 1
  print(paste("#### File", j))
  ent_j <- data.frame(V1 = names(dat_ent), V2=t(dat_ent[j,]))
  ent_j$order <- 1:nrow(ent_j)
  #title2 <- paste(c(j, "entity", as.character(ent_j[1,2])),
  #        collapse = ", ", sep = "")
  #  title2
    qc_ent <- merge(ent_j, help_ent, by.x = "V1", by.y = "datinv", all.x = T)
    qc_ent2 <- qc_ent[order(qc_ent$order),c(1,2,5)]
  #qc_ent2
  #qc_ent3 <- data.frame(index = rep(title2, nrow(qc_ent2)),
  #                      index = c(title2, rep("", nrow(qc_ent2) - 1)),
  #                      meta = qc_ent2)
  names(qc_ent2) <- c("variable","value","help_comment")
  qc_ent2[,2] <- gsub("\n", " | ", qc_ent2[,2])
  #print(kable(qc_ent2, row.names = F))
  
  print(xtable(qc_ent2), type = "html", include.rownames = F)
  #write.csv(qc_ent2, paste(dset, "_data_deposit_form.csv", sep = ""), row.names = F)
  }
  
  
    
  ```
  
#+end_src

#+RESULTS:


** load transformed
*** COMMENT load-code
#+name:load
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:load ####
  if(exists('ch'))   dbDisconnect(ch)
  library(swishdbtools)
  library(sqldf)
  library(knitr)
  
  #etl  <- read.csv("lastregen_data_deposit_form.csv", stringsAsFactor = F)
  #etl2 <- as.data.frame(t(etl[,"value"]))
  #names(etl2)  <- etl[,"variable"]
  #etl2
  ch  <- connect2postgres2("data_inventory_hanigan_dev")
  ch2 <- connect2postgres2("data_inventory_hanigan_dev3")
  #etl <- sqldf(connection = ch, "select * from dataset where shortname  = 'lastregen'")
  dbGetQuery(ch, "select id from dataset where shortname = 'lastregen'")
  
  etl <- sqldf(connection = ch, "select * from entity where dataset_id  = 23")
  etl
  dbRemoveTable(ch2, "temp")
  dbWriteTable(ch2, "temp", etl, row.names = F)
  
  
  
  
  prj <- etl2$shortname
  did <- dbGetQuery(ch,
                    sprintf("select id from dataset where shortname = '%s'", prj)
                            )
  did
  pid <- dbGetQuery(ch,
  sprintf("select id from project where title = '%s'", etl2$project_title)
                    )
  pid
  # project
  dbSendQuery(ch,
  #cat(
  paste("INSERT INTO project(
  id, title, personnel, abstract, studyareadescription)
  VALUES (",pid,",
  '",etl2$project_title,"',
  '",etl2$creator,"',
  '",etl2$abstract,"',
  '",etl2$studyareadescription,"');
  ",sep = ""
  )
  )
  
  # datasets
  
  
  
  # entity
  
  
  # keywords
  library(stringr)
  keywords <- unlist(lapply(strsplit(as.character(etl2$keywords), ","), str_trim))
  keywords
  
#+end_src
** summary of EML fields for projects/datasets 
*** head
#+begin_src R :session *R* :tangle EML_metadata_fields.Rmd :exports none :eval no :padline no
  ---
  title: EML metadata fields
  output: html_document
  ---
  
  
  
  # Introduction
  
  Date metadata document produced: `r Sys.Date()`.
  
  ```{r, echo = F, eval = T, results="hide"}
  #### name:summary of project info ####
  if(exists('ch'))   dbDisconnect(ch)
  library(swishdbtools)
  library(sqldf)
  library(knitr)
  library(xtable)  
  
  ch  <- connect2postgres2("data_inventory_hanigan_dev4")
  #prj <- "Air_Pollution_Monitoring_Stations_WA"
  #dset <- "AP_WA_1994_2012"
  
  #ch  <- connect2postgres2("data_inventory_noise")
  #prj <- "Noise Model"
  #dset <- "noise_model_preliminary_results"
  
  #ch  <- connect2postgres2("data_inventory_air_pollution")
  #prj <- "Walkability"
  #dset <- "walkability_mayne"
  
  
  show_exemplar <- T
  ```
  
#+end_src
*** dataset overview
#+begin_src R :session *R* :tangle EML_metadata_fields.Rmd :exports none :eval no :padline no
  
  # Dataset or study 'package' overview
  ```{r, echo = F, eval = T, results="asis"}
  
  #### for each dataset
  #dat$shortname
  # for(i in 1:nrow(dat)){
  
  # i = which(dat$shortname == dset)
  
  
  dat <- dbGetQuery(ch,
  sprintf("select shortname,title,
    creator,
    contact,
    contact_email,
    abstract,
    associated_party, recommended_citation,
    studyextent,
    geographicdescription,
    boundingcoordinates,
    temporalcoverage_daterange,
    temporalcoverage_begindate,
    temporalcoverage_enddate,
    methods_protocol,
    sampling_desc,
    method_steps,
    additional_metadata,
    taxonomic_coverage,
    additionalinfo,
    alternateidentifier,
    pubdate,
    metadataprovider,
    access_rules,
    distribution_methods
   from dataset
   where shortname ='%s'", dset)
  )
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
   #dat_i
   dat_i$order <- 1:nrow(dat_i)
    #title <- paste(c(as.character(dat_i[dat_i$V1 %in% c('shortname','title'),2])),
    #      collapse = ", ", sep = "")
    #title
  
  help  <- dbGetQuery(ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%dataset%'"
    )
  # head(help)
  
  
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  #qc[1,]
  #names(qc)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  #qc2
  #qc3 <- data.frame(index1 = rep(paste("0. dataset", dset), nrow(qc2)),
  #           index2 = c(title, rep("", nrow(qc2) - 1)),
  #           metadata = qc2)
  #names(qc3) <- c("index1", "index2", "variable", "value", "help")
  names(qc2) <- c("variable", "value", "help")
  #names(qc2)
  #### Keyword
  ky <- dbGetQuery(ch,
    #cat(q
    paste("select t3.keyword
    from dataset t1
    join keyword t3
    on t1.id = t3.dataset_id
    where t1.shortname = '",dset,"'
    ", sep = "")
  )
  
  if(nrow(ky) > 0){
  ky <- ky[,1]
  } else {
  ky <- ''
  }
  ky <- paste(ky, sep = "", collapse=", ")
  ky <- data.frame(variable = "keywords", value = ky, help="Keywords or phrases that concisely describe the resource. Example is biodiversity. Use a controlled vocabulary thesaurus")
  
  
  qc_out <- rbind(qc2, ky)
  #qc_out[,1:3]
  #qc_out
  
  #kable(qc_out, row.names = F)
  
  
  dat <- dbGetQuery(ch,
  sprintf("select t1.data_owner, data_owner_contact, licence_code, licence_text, licencee, path_to_licence, special_conditions
  from intellectualright t1
  join dataset t2
  on t1.dataset_id =  t2.id
   where shortname ='%s'", dset)
  )
  if(nrow(dat) == 0){
  dat <- data.frame(id = '', dataset_id = '', data_owner='', data_owner_contact='', licence_code = '',
    licence_text = '',licencee='', path_to_licence='', special_conditions='')
  }
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
   #dat_i
   dat_i$order <- 1:nrow(dat_i)
    #title <- paste(c(as.character(dat_i[dat_i$V1 %in% c('shortname','title'),2])),
    #      collapse = ", ", sep = "")
    #title
  
  help  <- sqldf(connection = ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%intellectualright%'"
    )
  # head(help)
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  names(qc2) <- c("variable", "value", "help")
  # names(  qc2)
  qc_out <- rbind(qc_out, qc2[-c(1,2),])
  qc_out[,2] <- gsub("\n", " | ", qc_out[,2])
  qc_out_raw <- qc_out
  qc_out <- qc_out_raw[qc_out_raw$variable %in% c('shortname','title',
  'creator',
  'contact',
  'contact_email',
  'abstract'),]
  if(show_exemplar){
  print(xtable(qc_out[,]), type = "html", include.rownames = F)
  } else {
  print(xtable(qc_out[,-2]), type = "html", include.rownames = F)
  }
  ```
#+end_src
*** umbrella project
#+begin_src R :session *R* :tangle EML_metadata_fields.Rmd :exports none :eval no :padline no
    
  # Umbrella project level information
  
  ```{r, echo = F, eval = T, results="asis"}  
  
  dat <- sqldf(connection = ch,
    sprintf("select t1.*
    from project t1
    where t1.title = '%s'", prj)
    )
  #names(dat)
  #t(dat)
  
  ####  help
  help  <- sqldf(connection = ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%project%'"
    )
  # head(help)
  
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
  #dat_i
  dat_i$order <- 1:nrow(dat_i)  
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  names(qc2) <- c("variable", "value", "help_comment")
  
  qc2[,2] <- gsub("\n", " | ", qc2[,2])
  if(show_exemplar){
  print(xtable(qc2[,]), type = "html", include.rownames = F)
  } else {
  print(xtable(qc2[,-2]), type = "html", include.rownames = F)
  }  
  ```
#+end_src
*** dataset overview
#+begin_src R :session *R* :tangle EML_metadata_fields.Rmd :exports none :eval no :padline no
  # Dataset or study detailed information   
  ```{r, echo = F, eval = T, results="asis"}  
  
  qc_out <- qc_out_raw[qc_out_raw$variable %in% c('recommended_citation',
  'associated_party',
  'geographicdescription',
  'boundingcoordinates',
  'temporalcoverage_daterange',
  'temporalcoverage_begindate',
  'temporalcoverage_enddate',
  'taxonomic_coverage',
  'method_steps',
  'additional_metadata',
  'additionalinfo',
  'alternateidentifier',
  'pubdate',
  'metadataprovider',
  'methods_protocol',
  'access_rules',
  'distribution_methods',
  'sampling_desc',
  'studyextent',
  'keywords',
  'licence_code',
  'licence_text','licencee','data_owner','data_owner_contact','path_to_licence',
  'special_conditions', 'recommended_citation'
  ),]
  if(show_exemplar){
  print(xtable(qc_out[,]), type = "html", include.rownames = F)
  } else {
  print(xtable(qc_out[,-2]), type = "html", include.rownames = F)
  }
  ```
  
#+end_src

*** files entities overview
#+begin_src R :session *R* :tangle EML_metadata_fields.Rmd :exports none :eval no :padline no
  
  # File / Entity level information 
  
  ```{r, echo = F, eval = T, results="asis"}
    
  #### entity ####
  #dat <- dbGetQuery(ch, "select * from entity")
  dat_ent <- dbGetQuery(ch,
  # cat(
  sprintf("select 
  t3.*
  from project t1
  join dataset t2
  on t1.id = t2.project_id
  join entity t3
  on t2.id = t3.dataset_id
  where t1.title = '%s'
  and t2.shortname = '%s'", prj, dset)
  )
  # head(dat_ent)
  
  help_ent  <- sqldf("select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%entity%'",
    connection = ch)
  #help_ent
  
  
  for(j in 1:nrow(dat_ent)){
  #j = 1
  print(paste("#### File", j))
  ent_j <- data.frame(V1 = names(dat_ent), V2=t(dat_ent[j,]))
  ent_j$order <- 1:nrow(ent_j)
  #title2 <- paste(c(j, "entity", as.character(ent_j[1,2])),
  #        collapse = ", ", sep = "")
  #  title2
    qc_ent <- merge(ent_j, help_ent, by.x = "V1", by.y = "datinv", all.x = T)
    qc_ent2 <- qc_ent[order(qc_ent$order),c(1,2,5)]
  #qc_ent2
  #qc_ent3 <- data.frame(index = rep(title2, nrow(qc_ent2)),
  #                      index = c(title2, rep("", nrow(qc_ent2) - 1)),
  #                      meta = qc_ent2)
  names(qc_ent2) <- c("variable","value","help_comment")
  qc_ent2[,2] <- gsub("\n", " | ", qc_ent2[,2])
  #print(kable(qc_ent2, row.names = F))
  if(show_exemplar){
  print(xtable(qc_ent2[,]), type = "html", include.rownames = F)
  } else {
  print(xtable(qc_ent2[,-2]), type = "html", include.rownames = F)
  }
  #write.csv(qc_ent2, paste(dset, "_data_deposit_form.csv", sep = ""), row.names = F)
  }
  
  
    
  ```
  
#+end_src
*** attributes
#+begin_src R :session *R* :tangle EML_metadata_fields.Rmd :exports none :eval no :padline no
  
  <!--# Variables attributes-->
  ```{r, echo = F, eval = T, results="asis"}
  dat <- sqldf(connection = ch,
    sprintf("select t1.* 
    from attr t1 
    where entity_id = %s", as.numeric(as.character(ent_j[1,2])))
    )
  #names(dat)
  #t(dat)
  if(length(nrow(dat) > 0) != 0 & nrow(dat) >0){
  ####  help
  help  <- sqldf(connection = ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%%'"
    )
  # head(help)
  
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
  #dat_i
  dat_i$order <- 1:nrow(dat_i)  
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  names(qc2) <- c("variable", "value", "help_comment")
  
  qc2[,2] <- gsub("\n", " | ", qc2[,2])
  ## if(show_exemplar){
  ## cat("Variables attributes
  ## ---
  ## ")
  ## print(xtable(qc2[-c(1,2),]), type = "html", include.rownames = F)
  ## } else {
  ## cat("Variables attributes
  ## ---
  ## ")
    
  ## print(xtable(qc2[-c(1,2),-2]), type = "html", include.rownames = F)
  ## }
  
  # actually give info then display
  cat("Variables attributes
  ---
  Instructions
  ")  
  print(xtable(qc2[-c(1,2),-2]), type = "html", include.rownames = F)
  cat("\nThe attributes are:\n")
  print(xtable(
    dat[,-1]
    ), type = "html", include.rownames = F)
  
  }
  ```
  
#+end_src
#+RESULTS:

** short summary of EML fields for projects/datasets homepage
*** head
#+begin_src R :session *R* :tangle EML_metadata_short.Rmd :exports none :eval no :padline no
  ---
  title: EML metadata fields
  output: html_document
  ---
  
  Date metadata document produced: `r Sys.Date()`.
  
  ```{r, echo = F, eval = T, results="hide"}
  #### name:summary of project info ####
  if(exists('ch'))   dbDisconnect(ch)
  library(swishdbtools)
  library(sqldf)
  library(knitr)
  library(xtable)  
  
  ch  <- connect2postgres2("data_inventory_hanigan_dev4")  

  #prj <- "Air_Pollution_Monitoring_Stations_WA"
  #dset <- "AP_WA_19942012"
  
  #ch  <- connect2postgres2("data_inventory_noise")
  #prj <- "Noise Model"
  #dset <- "noise_model_preliminary_results"
  
  #ch  <- connect2postgres2("data_inventory_air_pollution")
  #prj <- "Neighbourhood_Exposures_for_Cohort_Studies"
  #dset <- "Neighbourhood_exposures"
  
  
  #show_exemplar <- T
  ```
  
#+end_src
*** dataset overview
#+begin_src R :session *R* :tangle EML_metadata_short.Rmd :exports none :eval no :padline no
  
  
  ```{r, echo = F, eval = T, results="asis"}
  
  #### for each dataset
  #dat$shortname
  # for(i in 1:nrow(dat)){
  
  # i = which(dat$shortname == dset)
  
  
  dat <- dbGetQuery(ch,
  sprintf("select shortname,title,
    creator,
    contact,
    contact_email,
    abstract,
    additional_metadata,
    associated_party, recommended_citation,
    studyextent,
    geographicdescription,
    boundingcoordinates,
    temporalcoverage_daterange,
    temporalcoverage_begindate,
    temporalcoverage_enddate,
    methods_protocol,
    sampling_desc,
    method_steps,
  
    taxonomic_coverage,
    additionalinfo,
    alternateidentifier,
    pubdate,
    metadataprovider,
    access_rules,
    distribution_methods
   from dataset
   where shortname ='%s'", dset)
  )
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
  # dat_i
   dat_i$order <- 1:nrow(dat_i)
    #title <- paste(c(as.character(dat_i[dat_i$V1 %in% c('shortname','title'),2])),
    #      collapse = ", ", sep = "")
    #title
  
  help  <- dbGetQuery(ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%dataset%'"
    )
  # head(help)
  
  
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  #qc[1,]
  #names(qc)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  #qc2
  #qc3 <- data.frame(index1 = rep(paste("0. dataset", dset), nrow(qc2)),
  #           index2 = c(title, rep("", nrow(qc2) - 1)),
  #           metadata = qc2)
  #names(qc3) <- c("index1", "index2", "variable", "value", "help")
  names(qc2) <- c("variable", "value", "help")
  #names(qc2)
  #### Keyword
  ## ky <- dbGetQuery(ch,
  ##   #cat(q
  ##   paste("select t3.keyword
  ##   from dataset t1
  ##   join keyword t3
  ##   on t1.id = t3.dataset_id
  ##   where t1.shortname = '",dset,"'
  ##   ", sep = "")
  ## )
  
  ## if(nrow(ky) > 0){
  ## ky <- ky[,1]
  ## } else {
  ## ky <- ''
  ## }
  ## ky <- paste(ky, sep = "", collapse=", ")
  ## ky <- data.frame(variable = "keywords", value = ky, help="Keywords or phrases that concisely describe the resource. Example is biodiversity. Use a controlled vocabulary thesaurus")
  
  
  ## qc_out <- rbind(qc2, ky)
  qc_out <- qc2
  ## #qc_out[,1:3]
  #qc_out
  
  #kable(qc_out, row.names = F)
  
  
  dat <- dbGetQuery(ch,
  sprintf("select t1.id, dataset_id, data_owner, data_owner_contact, licence_code, licence_text, licencee, path_to_licence, special_conditions
  from intellectualright t1
  join dataset t2
  on t1.dataset_id =  t2.id
   where shortname ='%s'", dset)
  )
  if(nrow(dat) == 0){
  dat <- data.frame(id = '', dataset_id = '', data_owner='', data_owner_contact='', licence_code = '',
    licence_text = '',licencee='', path_to_licence='', special_conditions='')
  }
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
   #dat_i
   dat_i$order <- 1:nrow(dat_i)
    #title <- paste(c(as.character(dat_i[dat_i$V1 %in% c('shortname','title'),2])),
    #      collapse = ", ", sep = "")
    #title
  
  help  <- sqldf(connection = ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%intellectualright%'"
    )
  # head(help)
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  names(qc2) <- c("variable", "value", "help")
  # names(  qc2)
  qc_out <- rbind(qc_out, qc2[-c(1,2),])
  qc_out[,2] <- gsub("\n", " | ", qc_out[,2])
  qc_out_raw <- qc_out
  qc_out <- qc_out_raw[qc_out_raw$variable %in% c('shortname','title',
  'creator',
  'contact',
  'contact_email',
  'abstract'),]
  #if(show_exemplar){
  #print(xtable(qc_out[,]), type = "html", include.rownames = F)
  #} else {
  #print(xtable(qc_out[,-2]), type = "html", include.rownames = F)
  #}
  ```
#+end_src
*** umbrella project
#+begin_src R :session *R* :tangle EML_metadata_short.Rmd :exports none :eval no :padline no
    

  
  ```{r, echo = F, eval = T, results="asis"}  
  
  dat <- sqldf(connection = ch,
    sprintf("select t1.title,  personnel_data_owner
    from project t1
    where t1.title = '%s'", prj)
    )
  #names(dat)
  #t(dat)
  
  ####  help
  help  <- sqldf(connection = ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%project%'"
    )
  # head(help)
  
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
  #dat_i
  dat_i$order <- 1:nrow(dat_i)  
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  names(qc2) <- c("variable", "value", "help")
  qc2[,1] <- gsub("title", "umbrella_project_title", qc2[,1])
  qc2[,1] <- gsub("personnel_data_owner", "project_owner_or_originator", qc2[,1])  
  qc2[,2] <- gsub("\n", " | ", qc2[,2])
  qc2 <- rbind(qc_out, qc2)
  
  
  ## if(show_exemplar){
  ## print(xtable(qc2[,]), type = "html", include.rownames = F)
  ## } else {
  ## print(xtable(qc2[,-2]), type = "html", include.rownames = F)
  ## }  
  ```
#+end_src
*** dataset overview
#+begin_src R :session *R* :tangle EML_metadata_short.Rmd :exports none :eval no :padline no
  # Dataset or study metadata   
  ```{r, echo = F, eval = T, results="asis"}  
  
  qc_out <- qc_out_raw[qc_out_raw$variable %in% c('recommended_citation',
  'additional_metadata',
  'studyextent',
  'temporalcoverage_daterange',
  'methods_protocol',
  'sampling_desc',
  'method_steps',
  'associated_party',
  'access_rules',
  'distribution_methods',
  'keywords',
  'licence_code',
  'licence_text','path_to_licence','licencee',
  'data_owner',
  'data_owner_contact',
  'special_conditions'
  ),]
  qc_out <- rbind(qc2, qc_out)

  if(show_exemplar){
  print(xtable(qc_out[,]), type = "html", include.rownames = F)
  } else {
  print(xtable(qc_out[,-2]), type = "html", include.rownames = F)
  }
  ```
  
#+end_src

*** files entities overview
EML_metadata_short.Rmd
#+begin_src R :session *R* :tangle EML_metadata_short.Rmd :exports none :eval no :padline no
  
  # File / Entity level information 
  
  ```{r, echo = F, eval = T, results="asis"}

  #### entity ####
  #dat <- dbGetQuery(ch, "select * from entity")
  dat_ent <- dbGetQuery(ch,
  #cat(
  sprintf("select 
  t3.*
  from project t1
  join dataset t2
  on t1.id = t2.project_id
  join entity t3
  on t2.id = t3.dataset_id
  where t1.title = '%s'
  and t2.shortname = '%s'", prj, dset)
  )
  
  if(nrow(dat_ent) == 0){

   print("No files documented yet")

  } else {
  
  if(entities){    
  help_ent  <- sqldf("select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%entity%'",
    connection = ch)
  #help_ent
  
  
  for(j in 1:nrow(dat_ent)){
  #j = 1
  print(paste("#### File", j))
  ent_j <- data.frame(V1 = names(dat_ent), V2=t(dat_ent[j,]))
  ent_j$order <- 1:nrow(ent_j)
  #title2 <- paste(c(j, "entity", as.character(ent_j[1,2])),
  #        collapse = ", ", sep = "")
  #  title2
    qc_ent <- merge(ent_j, help_ent, by.x = "V1", by.y = "datinv", all.x = T)
    qc_ent2 <- qc_ent[order(qc_ent$order),c(1,2,5)]
  #qc_ent2
  #qc_ent3 <- data.frame(index = rep(title2, nrow(qc_ent2)),
  #                      index = c(title2, rep("", nrow(qc_ent2) - 1)),
  #                      meta = qc_ent2)
  names(qc_ent2) <- c("variable","value","help")
  qc_ent2[,2] <- gsub("\n", " | ", qc_ent2[,2])
  #print(kable(qc_ent2, row.names = F))
  if(show_exemplar){
  print(xtable(qc_ent2[-c(1,2),]), type = "html", include.rownames = F)
  } else {
  print(xtable(qc_ent2[-c(1,2),-2]), type = "html", include.rownames = F)
  }
  #write.csv(qc_ent2, paste(dset, "_data_deposit_form.csv", sep = ""), row.names = F)
  }
  
  
  } else {
  print(xtable(dat_ent[,c("entityname","entitydescription","physical_distribution")]), type = "html", include.rownames = F)
  }   

  }
  ```
  
#+end_src

** data inventory report
*** COMMENT go-code
#+name:go
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:go ####
  projdir  <- "/home/ivan_hanigan/tools/web2py/applications/data_inventory/static"
  setwd(projdir)
  dir()
  do_loops <- TRUE
  rmarkdown::render("data_inventory.Rmd", "html_document")
  dbDisconnect(ch)
  #  browseURL("data_inventory.html")
  
#+end_src

#+RESULTS: go
: TRUE

*** COMMENT RMD
#+begin_src R :session *R* :tangle data_inventory.Rmd :exports none :eval no
  ---
  title: "Project and data inventory"
  author: Ivan Hanigan
  output:
    html_document:
       toc: true
  ---
  
   
  # Introduction
  
  - Updated: `r Sys.Date()`
      
  
  # Data Inventory of Potential Data Sources  
  
  ```{r, echo = F, results = 'hide', warning=FALSE, message=FALSE }  
  library(swishdbtools)
  library(sqldf)
  library(knitr)
  library(xtable)  
  ```
  
  ```{r, echo = F, results = 'asis'}
  if(do_loops){
  ch  <- connect2postgres2("data_inventory_hanigan_dev4")
  projects <- dbGetQuery(ch, "select title, project_abstract, personnel_data_owner
  from project
  order by title")
  #print(projects$title)
  for(i in 1:nrow(projects)){
  #i = 14
  prj <- projects[i,1]
  txt <- sprintf('# %s\n\n', prj)
  
  ## txt <- paste(txt,
  ##              sprintf("- Abstract: %s\n- Data Owner: %s\n",
  ##                      projects[i,2],
  ##                      projects[i,3]
  ##                      ),
  ##              sep = "")
  cat(txt)
    qc <- as.data.frame(t(projects[i,c("title", "personnel_data_owner", "project_abstract")]))
    names(qc) <- c("metadata_field")
    qc[,1] <- gsub("\n", " | ", qc[,1])
    print(xtable(qc), include.rownames = T, type = "html")
    cat("\n\n")
    
  
  datasets <- dbGetQuery(ch,
  sprintf("select t1.*
  from dataset t1
  join project t2
  on t1.project_id = t2.id
  where t2.title = '%s' order by t1.shortname", gsub("'", "\\'", prj))
                         )
  #str(datasets)
  if(
  nrow(datasets)==0
  ) next
  for(j in 1:nrow(datasets)){
  #j = 1
    dset <- datasets$shortname[j]
  #print(dset)
  txt <- sprintf('\n## %s\n\n', dset)
    
   
  ## txt <- paste(txt,
  ##              sprintf("- Dataset Abstract: %s\n- Creator: %s\n- Contact: %s\n\n",
  ##                      datasets[j,"abstract"],
  ##                      datasets[j,"creator"],
  ##                      datasets[j,"contact"]                     
  ##                      )
  ##              , sep = "")
  cat(txt)
    qc <- as.data.frame(t(datasets[j,c("shortname", "title", "creator", "contact", "abstract", "additional_metadata", "studyextent", "temporalcoverage_daterange", "methods_protocol", "sampling_desc", "method_steps", "access_rules")]))
    names(qc) <- c("metadata_field")
    qc[,1] <- gsub("\n", " | ", qc[,1])
    print(xtable(qc), include.rownames = T, type = "html")
    cat("\n\n")    
  
    #entities <- dbGetQuery(ch,
    #sprintf("select t1.entityname, entitydescription, physical_distribution
    #from entity t1
    #join dataset t2
    #on t1.dataset_id = t2.id
    #where t2.shortname = '%s'", dset)
    #                       )
    # str(entities)
  
    #cat("\n### Files and Folders\n\n")
    #print(xtable(entities), type = "html", include.rownames = F)
  
  ## for(k in 1:nrow(entities)){
  ##   #k = 1
  ##   ent <- entities[k,"entityname"]
  ##   txt <- sprintf('\n### %s\n\n', ent)
  ##   cat(txt)
  ##   qc <- as.data.frame(t(entities[k,-c(1:2)]))
  ##   names(qc) <- NULL
  ##   qc[,1] <- gsub("\n", " | ", qc[,1])
  ##   print(xtable(qc), include.rownames = T, type = "html")
  ##   cat("\n\n")  
  ## }
  
  }
  }
  } 
  ```
  
#+end_src
* COMMENT check in/check out
** checkout from child to master when master not exists
#+begin_src R :session *R* :tangle no :exports none :eval no
  require(swishdbtools)
  ch_source <- connect2postgres2("data_inventory_air_pollution_local")  
  ch_target <- connect2postgres2("data_inventory_hanigan_dev4")  
  tbls <- pgListTables(ch_source, "public")
  tbls$no <- 1:nrow(tbls)
  tbls
  #tbls[c(1:4,9,12:21),1]
  #tbls <- tbls[c(1:4,9,12:21),1]
  
  # get the project id
  proj <- "45_and_Up_Study" 
  proj_dat  <- dbGetQuery(ch_source,
                          paste("select * from project where title = '",proj,"'", sep = "")
                          )
  pid  <- proj_dat$id
  proj_dat
  # check not existing
  nrow_target  <- dbGetQuery(ch_target,
                             paste("select * from project where title like '%",proj,"%'", sep = "")
                             )
  nrow_target
  # if OK then write and return the new pid
  if(
    nrow(nrow_target) == 0
    ){
  dbWriteTable(ch_target, "project_insert", proj_dat)
  dbSendQuery(ch_target,
              "INSERT INTO project(
              title, personnel_data_owner, personnel_owner_organisationname, 
              personnel, funding, project_abstract, studyareadescription, project_established, 
              project_citation, related_project)
              select title, personnel_data_owner, personnel_owner_organisationname, 
              personnel, funding, project_abstract, studyareadescription, project_established, 
              project_citation, related_project
              from project_insert")
  pid2 <- dbGetQuery(ch_target,
                     paste("select id from project where title = '",proj,"'", sep = "")
                     )
  dbRemoveTable(ch_target, "project_insert")  
  } else {
    pid2  <- dbGetQuery(ch_target,
                             paste("select id from project where title like '%",proj,"%'", sep = "")
                             )
    sprintf("already exists! and Proj ID is %s", pid2)
  }
  
  
  # get datasets
  dsets  <- dbGetQuery(ch_source,
                       paste("select * from dataset where project_id = ",pid, sep = "")
                       )
  dsets[,1:4]
  
  # recursively walk back up from attr and licence
  # we are not using publs at the moment for UCRH, but that is future
  # possible
  tbls
  ##                relname nspname no
  ## 18       accessdataset  public  1
  ## 16            accessor  public  2
  ## 17       accessrequest  public  3
  ## 20            approval  public  4
  ## 21                attr  public  5
  ## 3             auth_cas  public  6
  ## 2           auth_event  public  7
  ## 1           auth_group  public  8
  ## 22     auth_membership  public  9
  ## 14 authorship_approval  public 10
  ## 5      auth_permission  public 11
  ## 10           auth_user  public 12
  ## 4         bibliometric  public 13
  ## 13           checklist  public 14
  ## 7            crosswalk  public 15
  ## 8           crosswalk2  public 16
  ## 23             dataset  public 17
  ## 6               entity  public 18
  ## 9                error  public 19
  ## 11   intellectualright  public 20
  ## 12             keyword  public 21
  ## 19             project  public 22
  ## 24         publication  public 23
  ## 15     thesaurus_ltern  public 24
  
  # loop over dsets
  dsets[1,]
  dsets[,1:3]
  for(i in 1:nrow(dsets)){
  #i = 2
    # grab d_id for use when extracting from entity and licence
    d_id <- dsets$id[i]
    d_id
  
  dbWriteTable(ch_target, "dataset_insert", dsets[i,])
  dbSendQuery(ch_target,
              #cat(
              paste(
              "INSERT INTO dataset(
              project_id, shortname, creator, contact, contact_email, abstract, 
              geographicdescription, boundingcoordinates, temporalcoverage_daterange, 
              temporalcoverage_begindate, temporalcoverage_enddate, taxonomic_coverage, 
              method_steps, additional_metadata, additionalinfo, alternateidentifier, 
              pubdate, metadataprovider, title, access_rules, sampling_desc, 
              studyextent, associated_party, methods_protocol, distribution_methods)
              select ",pid2, ", shortname, creator, contact, contact_email, abstract, 
              geographicdescription, boundingcoordinates, temporalcoverage_daterange, 
              temporalcoverage_begindate, temporalcoverage_enddate, taxonomic_coverage, 
              method_steps, additional_metadata, additionalinfo, alternateidentifier, 
              pubdate, metadataprovider, title, access_rules, sampling_desc, 
              studyextent, associated_party, methods_protocol, distribution_methods
              from dataset_insert", sep = "")
              )
  d_id2 <- dbGetQuery(ch_target,
                     "select max(id) from dataset"
                     )
  d_id2
  dbRemoveTable(ch_target, "dataset_insert")  
  
  #tbls_todo <- c("entity","intellectualright")  
  #for(tbl_i in tbls_todo){
  #  tbl_i = tbls_todo[1]
  tbl_i <- "entity"
  todo_i <- dbGetQuery(ch_source,
                    sprintf("select * from public.%s where dataset_id = %s", tbl_i, d_id)
                    )
  todo_i
  if(nrow(todo_i) > 0){    
  dbWriteTable(ch_target, "to_insert", todo_i)
  dbSendQuery(ch_target,
              paste("INSERT INTO entity(
              dataset_id, entityname, entitydescription, entity_methods, 
              physical_distribution, physical_distribution_additionalinfo, 
              numberofrecords, entity_temporalcoverage_daterange)
              select ",d_id2,", entityname, entitydescription, entity_methods, 
              physical_distribution, physical_distribution_additionalinfo, 
              numberofrecords, entity_temporalcoverage_daterange
              from to_insert", sep = "")
              )
  dbRemoveTable(ch_target, "to_insert")
  }
  tbl_i <- "intellectualright"
  todo_i <- dbGetQuery(ch_source,
                    sprintf("select * from public.%s where dataset_id = %s", tbl_i, d_id)
                    )
  todo_i
  if(nrow(todo_i) > 0){  
  dbWriteTable(ch_target, "to_insert", todo_i)
  dbSendQuery(ch_target,
              paste("INSERT INTO intellectualright(
              dataset_id, licence_code, licence_text, path_to_licence, 
              data_owner_contact, data_owner, special_conditions, licencee)            
              select ",d_id2,", licence_code, licence_text, path_to_licence, 
              data_owner_contact, data_owner, special_conditions, licencee
              from to_insert", sep = "")
              )
  dbRemoveTable(ch_target, "to_insert")
  }
  }
  # NOW MANUALLY REMOVE THE SOURCE VERSION SO THAT MASTER IS NOW IN TARGET DB
#+end_src
** checkout from master to child, assumes you want to replace child (and manually removed it)
#+begin_src R :session *R* :tangle no :exports none :eval no
  require(swishdbtools)
  ch_source <- connect2postgres2("data_inventory_hanigan_dev4")  
  ch_target <- connect2postgres2("data_inventory_air_pollution_local")  
  tbls <- pgListTables(ch_source, "public")
  tbls$no <- 1:nrow(tbls)
  tbls
  #tbls[c(1:4,9,12:21),1]
  #tbls <- tbls[c(1:4,9,12:21),1]
  
  # get the project id
  proj <- "CTM_CSIRO"
  proj_dat  <- dbGetQuery(ch_source,
                          paste("select * from project where title = '",proj,"'", sep = "")
                          )
  pid  <- proj_dat$id
  t(proj_dat)
  # check not existing
  nrow_target  <- dbGetQuery(ch_target,
                             paste("select * from project where title = '",proj,"'", sep = "")
                             )
  nrow_target
  # if OK then write and return the new pid
  if(
    nrow(nrow_target) == 0
    ){
  dbWriteTable(ch_target, "project_insert", proj_dat)
  dbSendQuery(ch_target,
              "INSERT INTO project(
              id, title, personnel_data_owner, personnel_owner_organisationname, 
              personnel, funding, project_abstract, studyareadescription, project_established, 
              project_citation, related_project)
              select id, title, personnel_data_owner, personnel_owner_organisationname, 
              personnel, funding, project_abstract, studyareadescription, project_established, 
              project_citation, related_project
              from project_insert")
  pid2 <- dbGetQuery(ch_target,
                     paste("select id from project where title = '",proj,"'", sep = "")
                     )
  dbRemoveTable(ch_target, "project_insert")  
  } else {
    print("already exists!")
  }
  
  
  # get datasets
  dsets  <- dbGetQuery(ch_source,
                       paste("select * from dataset where project_id = ",pid, sep = "")
                       )
  dsets[,1:3]
  
  # recursively walk back up from attr and licence
  # we are not using publs at the moment for UCRH, but that is future
  # possible
  tbls
  ##                relname nspname no
  ## 18       accessdataset  public  1
  ## 16            accessor  public  2
  ## 17       accessrequest  public  3
  ## 20            approval  public  4
  ## 21                attr  public  5
  ## 3             auth_cas  public  6
  ## 2           auth_event  public  7
  ## 1           auth_group  public  8
  ## 22     auth_membership  public  9
  ## 14 authorship_approval  public 10
  ## 5      auth_permission  public 11
  ## 10           auth_user  public 12
  ## 4         bibliometric  public 13
  ## 13           checklist  public 14
  ## 7            crosswalk  public 15
  ## 8           crosswalk2  public 16
  ## 23             dataset  public 17
  ## 6               entity  public 18
  ## 9                error  public 19
  ## 11   intellectualright  public 20
  ## 12             keyword  public 21
  ## 19             project  public 22
  ## 24         publication  public 23
  ## 15     thesaurus_ltern  public 24
  
  # loop over dsets
  dsets[1,]
  for(i in 2:nrow(dsets)){
  #i = 1
    # grab d_id for use when extracting from entity and licence
    d_id <- dsets$id[i]
    d_id
  
  dbWriteTable(ch_target, "dataset_insert", dsets[i,])
  dbSendQuery(ch_target,
              paste(
              "INSERT INTO dataset(
              id, project_id, shortname, creator, contact, contact_email, abstract, 
              geographicdescription, boundingcoordinates, temporalcoverage_daterange, 
              temporalcoverage_begindate, temporalcoverage_enddate, taxonomic_coverage, 
              method_steps, additional_metadata, additionalinfo, alternateidentifier, 
              pubdate, metadataprovider, title, access_rules, sampling_desc, 
              studyextent, associated_party, methods_protocol, distribution_methods)
              select id, project_id, shortname, creator, contact, contact_email, abstract, 
              geographicdescription, boundingcoordinates, temporalcoverage_daterange, 
              temporalcoverage_begindate, temporalcoverage_enddate, taxonomic_coverage, 
              method_steps, additional_metadata, additionalinfo, alternateidentifier, 
              pubdate, metadataprovider, title, access_rules, sampling_desc, 
              studyextent, associated_party, methods_protocol, distribution_methods
              from dataset_insert", sep = "")
              )
  ## d_id2 <- dbGetQuery(ch_target,
  ##                    "select max(id) from dataset"
  ##                    )
  ## d_id2
  dbRemoveTable(ch_target, "dataset_insert")  
  
  #tbls_todo <- c("entity","intellectualright")  
  #for(tbl_i in tbls_todo){
  #  tbl_i = tbls_todo[1]
  tbl_i <- "entity"
  todo_i <- dbGetQuery(ch_source,
                    sprintf("select * from public.%s where dataset_id = %s", tbl_i, d_id)
                    )
  todo_i
  dbWriteTable(ch_target, "to_insert", todo_i)
  dbSendQuery(ch_target,
              paste("INSERT INTO entity(
              id, dataset_id, entityname, entitydescription, entity_methods, 
              physical_distribution, physical_distribution_additionalinfo, 
              numberofrecords, entity_temporalcoverage_daterange)
              select id, dataset_id, entityname, entitydescription, entity_methods, 
              physical_distribution, physical_distribution_additionalinfo, 
              numberofrecords, entity_temporalcoverage_daterange
              from to_insert", sep = "")
              )
  dbRemoveTable(ch_target, "to_insert")
  
  tbl_i <- "intellectualright"
  todo_i <- dbGetQuery(ch_source,
                    sprintf("select * from public.%s where dataset_id = %s", tbl_i, d_id)
                    )
  todo_i
  if(nrow(todo_i) > 0){    
  dbWriteTable(ch_target, "to_insert", todo_i)
  dbSendQuery(ch_target,
              paste("INSERT INTO intellectualright(
              id, dataset_id, licence_code, licence_text, path_to_licence, 
              data_owner_contact, data_owner, special_conditions, licencee)            
              select id, dataset_id, licence_code, licence_text, path_to_licence, 
              data_owner_contact, data_owner, special_conditions, licencee
              from to_insert", sep = "")
              )
  dbRemoveTable(ch_target, "to_insert")
  }
  }
  
  # are the sequences out of sync?
  for(tabi in tbls$relname){
    maxi <- dbGetQuery(ch_target, sprintf("select max(id) from %s", tabi))
    print(data.frame(table = tabi, max = maxi))
  }
  for(tabi in tbls$relname){
    maxi <- dbGetQuery(ch_source, sprintf("select max(id) from %s", tabi))
    print(data.frame(table = tabi, max = maxi))  
  }
  
#+end_src
** DEPRECATED identify-aquire-post-update-review procedures
- doco for identified data are in a sep database so not to mess up clean inventory
- once data aquisitioon has started transfer to the main data inventory
*** COMMENT proj
#+name:proj
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:proj ####
  library(swishdbtools)
  if(exists('ch_in'))   dbDisconnect(ch_in)
  if(exists('ch_out'))   dbDisconnect(ch_out)
  
  remove_from_source  <- FALSE
  #ch_in  <- connect2postgres2("data_inventory_noise")
  #ch_in  <- connect2postgres2("data_inventory_air_pollution")
  ch_in  <- connect2postgres2("data_inventory_hanigan_dev4")
  # ch_out  <- connect2postgres2("data_inventory_hanigan_dev4")
  ch_out  <- connect2postgres2("data_inventory_air_pollution")
  
  prj <- "Air pollution modelling: LUR_Western_Sydney"
  dat <- dbGetQuery(ch_in,
    sprintf("select *
    from project t1
    where t1.title = '%s'", prj)
    )
  dat
  # check if proj title alread exists (a new id will be assigned)
  qc <- dbGetQuery(ch_out,
        sprintf("select * from project where title = '%s'", dat$title)
        )
  qc
  
  #### GO 
  if(
    nrow(qc) == 0
    ){
  dbWriteTable(ch_out, "foo", dat, row.names = F)
  
  # create inserts
  namlist  <- paste(names(dat)[-1], sep = '', collapse = ", ")        
  sql <- sprintf("insert into project (%s) select %s from foo",
  namlist, namlist)  
  cat(sql)
  dbSendQuery(ch_out, sql)
  dbRemoveTable(ch_out, "foo")
  
  datasets <- dbGetQuery(ch_in,
    sprintf("select t1.*
    from dataset t1
    join project t2
    on t1.project_id = t2.id
    where t2.title = '%s'", gsub("'", "\\'", prj))
                           )
  datasets[,"shortname"]
  dbWriteTable(ch_out, "foo", datasets, row.names = F)
  # create inserts
  newid <- dbGetQuery(ch_out, sprintf("select id from project where title = '%s'", dat$title))
  namlist  <- paste(names(datasets)[-c(1)], sep = '', collapse = ", ")
  namlist2  <- paste(names(datasets)[-c(1:2)], sep = '', collapse = ", ")
  sql <- sprintf("insert into dataset (%s) select '%s', %s from foo",
  namlist, newid, namlist2)  
  cat(sql)
  dbSendQuery(ch_out, sql)
  dbRemoveTable(ch_out, "foo")
  
  #### Stopped halfway as need to loop over datasets
  ## entities <- dbGetQuery(ch_in,
  ## #                       cat(
  ## sprintf("select t3.*
  ##   from entity t3 join
  ##   (select t1.*
  ##   from dataset t1
  ##   join project t2
  ##   on t1.project_id = t2.id
  ##   where t2.title = '%s') foo
  ##   on t3.dataset_id = foo.id", gsub("'", "\\'", prj))
  ##                          )
  ## entities
  ## dbWriteTable(ch_out, "foo", datasets, row.names = F)
  ## # create inserts
  ## newid <- dbGetQuery(ch_out, sprintf("select id from project where title = '%s'", dat$title))
  ## newid
  ## namlist  <- paste(names(datasets)[-c(1)], sep = '', collapse = ", ")
  ## namlist2  <- paste(names(datasets)[-c(1:2)], sep = '', collapse = ", ")
  ## sql <- sprintf("insert into dataset (%s) select '%s', %s from foo",
  ## namlist, newid, namlist2)  
  ## cat(sql)
  ## dbSendQuery(ch_out, sql)
  ## dbRemoveTable(ch_out, "foo")
  
  
  
  
  
  
  if(remove_from_source){
  dbSendQuery(ch_in, sprintf("delete from project where title = '%s'", dat$title))
  }
  
  } else {
    print("that title already exists")
  }
  
  
#+end_src
